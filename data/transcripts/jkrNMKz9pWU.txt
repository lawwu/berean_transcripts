
[00:00:00.000 --> 00:00:10.080]   Hi, I am Jeremy Howard from fast.ai, and this is a hacker's guide to language models.
[00:00:10.080 --> 00:00:15.100]   When I say a hacker's guide, what we're going to be looking at is a code-first approach
[00:00:15.100 --> 00:00:20.220]   to understanding how to use language models in practice.
[00:00:20.220 --> 00:00:25.940]   So before we get started, we should probably talk about what is a language model.
[00:00:25.940 --> 00:00:33.660]   I would say that this is going to make more sense if you know the kind of basics of deep
[00:00:33.660 --> 00:00:35.480]   learning.
[00:00:35.480 --> 00:00:39.100]   If you don't, I think you'll still get plenty out of it, and there'll be plenty of things
[00:00:39.100 --> 00:00:41.160]   you can do.
[00:00:41.160 --> 00:00:46.500]   But if you do have a chance, I would recommend checking out course.fast.ai, which is a free
[00:00:46.500 --> 00:00:56.340]   course, and specifically if you could at least kind of watch, if not work through, the first
[00:00:56.340 --> 00:00:57.340]   five lessons.
[00:00:57.340 --> 00:01:02.560]   That would get you to a point where you understand all the basic fundamentals of deep learning
[00:01:02.560 --> 00:01:09.820]   that will make this lesson tutorial make even more sense.
[00:01:09.820 --> 00:01:13.640]   Maybe I shouldn't call this a tutorial, it's more of a quick run-through.
[00:01:13.640 --> 00:01:19.140]   I'm going to try to run through all the basic ideas of language models, how to use them,
[00:01:19.140 --> 00:01:22.740]   both open source ones and open AI based ones.
[00:01:22.740 --> 00:01:27.820]   And it's all going to be based using code as much as possible.
[00:01:27.820 --> 00:01:32.660]   So let's start by talking about what a language model is.
[00:01:32.660 --> 00:01:36.740]   And so as you might have heard before, a language model is something that knows how to predict
[00:01:36.740 --> 00:01:42.220]   the next word of a sentence, or knows how to fill in the missing words of a sentence.
[00:01:42.220 --> 00:01:44.540]   And we can look at an example of one.
[00:01:44.540 --> 00:01:52.980]   Open AI has a language model, TextDaVinci003, and we can play with it by passing in some
[00:01:52.980 --> 00:01:57.300]   words and ask it to predict what the next words might be.
[00:01:57.300 --> 00:02:02.420]   So if we pass in "when I arrived back at the panda breeding facility after the extraordinary
[00:02:02.420 --> 00:02:05.580]   rain of live frogs, I couldn't believe what I saw.
[00:02:05.580 --> 00:02:08.940]   I just came up with that yesterday and I thought what might happen next."
[00:02:08.940 --> 00:02:12.420]   So kind of fun for creative brainstorming.
[00:02:12.420 --> 00:02:15.940]   There's a nice site called Nat.dev.
[00:02:15.940 --> 00:02:19.500]   Nat.dev lets us play with a variety of language models.
[00:02:19.500 --> 00:02:26.900]   And here I've selected TextDaVinci003, and I'll hit submit, and it starts printing stuff
[00:02:26.900 --> 00:02:27.900]   up.
[00:02:27.900 --> 00:02:31.580]   "The pandas were happily playing and eating the frogs that had fallen from the sky.
[00:02:31.580 --> 00:02:37.060]   It was an amazing sight to see these animals take advantage of such a unique opportunity.
[00:02:37.060 --> 00:02:40.860]   The staff took quick measures to ensure the safety of the pandas and the frogs."
[00:02:40.860 --> 00:02:43.940]   So there you go, that's what happened after the extraordinary rain of live frogs at the
[00:02:43.940 --> 00:02:46.260]   panda breeding facility.
[00:02:46.260 --> 00:02:52.500]   You'll see here that I've enabled show probabilities, which is a thing in Nat.dev, where it shows...
[00:02:52.500 --> 00:02:54.860]   Well, let's take a look.
[00:02:54.860 --> 00:02:57.960]   It's pretty likely the next word here is going to be "the".
[00:02:57.960 --> 00:03:00.700]   And after "the", since we're talking about a panda breeding facility, it's going to be
[00:03:00.700 --> 00:03:01.700]   "pandas were".
[00:03:01.700 --> 00:03:03.540]   And what were they doing?
[00:03:03.540 --> 00:03:05.020]   Well they could have been doing a few things.
[00:03:05.020 --> 00:03:07.620]   They could have been doing something happily.
[00:03:07.620 --> 00:03:11.900]   Or the pandas were having, the pandas were out, the pandas were playing.
[00:03:11.900 --> 00:03:14.420]   So it picked the most likely.
[00:03:14.420 --> 00:03:17.100]   It thought it was 20% likely it was going to be happily.
[00:03:17.100 --> 00:03:19.500]   And what were they happily doing?
[00:03:19.500 --> 00:03:26.340]   Could have been playing, hopping, eating, and so forth.
[00:03:26.340 --> 00:03:31.140]   So they're eating the frogs that, and then had, almost certainly.
[00:03:31.140 --> 00:03:35.880]   So you can see what it's doing at each point is it's predicting the probability of a variety
[00:03:35.880 --> 00:03:37.760]   of possible next words.
[00:03:37.760 --> 00:03:44.100]   And depending on how you set it up, it will either pick the most likely one every time,
[00:03:44.100 --> 00:03:52.460]   or you can change, muck around with things like p-values and temperatures to change what
[00:03:52.460 --> 00:03:53.700]   comes up.
[00:03:53.700 --> 00:03:58.820]   So at each time then it'll give us a different result.
[00:03:58.820 --> 00:04:03.660]   And this is kind of fun.
[00:04:03.660 --> 00:04:05.500]   Frogs perched on the heads of some of the pandas.
[00:04:05.500 --> 00:04:09.580]   It was an amazing sight, etc, etc.
[00:04:09.580 --> 00:04:18.540]   Okay, so that's what a language model does.
[00:04:18.540 --> 00:04:23.980]   Now you might notice here it hasn't predicted "pandas".
[00:04:23.980 --> 00:04:29.140]   It's predicted "panned", and then separately "us".
[00:04:29.140 --> 00:04:32.220]   Okay, after "panned" it's going to be "us".
[00:04:32.220 --> 00:04:34.740]   So it's not always a whole word.
[00:04:34.740 --> 00:04:37.940]   Here it's "un" and then "harmed".
[00:04:37.940 --> 00:04:41.780]   Oh, actually it's "unharmed".
[00:04:41.780 --> 00:04:44.700]   So you can see that it's not always predicting words.
[00:04:44.700 --> 00:04:48.100]   Specifically what it's doing is predicting tokens.
[00:04:48.100 --> 00:04:54.660]   Tokens are either whole words or sub-word units, pieces of a word, or it could even
[00:04:54.660 --> 00:05:00.700]   be punctuation or numbers or so forth.
[00:05:00.700 --> 00:05:02.540]   So let's have a look at how that works.
[00:05:02.540 --> 00:05:08.460]   So for example, we can use the actual, it's called tokenization, to create tokens from
[00:05:08.460 --> 00:05:10.740]   a string.
[00:05:10.740 --> 00:05:17.100]   We can use the same tokenizer that GPT uses by using TickToken, and we can specifically
[00:05:17.100 --> 00:05:23.220]   say we want to use the same tokenizer that that model, TextEventually003, uses.
[00:05:23.220 --> 00:05:28.980]   And so for example, when I earlier tried this, it talked about the frog splashing, and so
[00:05:28.980 --> 00:05:35.780]   I thought we'll encode "they are splashing", and the result is a bunch of numbers.
[00:05:35.780 --> 00:05:41.020]   And what those numbers are, they're basically just lookups into a vocabulary that OpenAI
[00:05:41.020 --> 00:05:45.000]   in this case created, and if you train your own models you'll be automatically creating
[00:05:45.000 --> 00:05:46.940]   or your code will create.
[00:05:46.940 --> 00:05:57.940]   And if I then decode those, it says, oh, these numbers are "they space are space spl ashing".
[00:05:57.940 --> 00:06:00.200]   And so put that all together, they are splashing.
[00:06:00.200 --> 00:06:08.340]   So you can see that the start of a word is, with a space before it, is also being encoded
[00:06:08.340 --> 00:06:12.260]   here.
[00:06:12.260 --> 00:06:23.300]   So these language models are quite neat, that they can work at all, but they're not of themselves
[00:06:23.300 --> 00:06:28.220]   really designed to do anything.
[00:06:28.220 --> 00:06:30.900]   Let me explain.
[00:06:30.900 --> 00:06:43.140]   The basic idea of what chat-gpt, gpt4, BARD, etc. are doing comes from a paper which describes
[00:06:43.140 --> 00:06:47.820]   an algorithm that I created back in 2017 called ULMfit.
[00:06:47.820 --> 00:06:52.060]   And Sebastian Ruder and I wrote a paper up describing the ULMfit approach, which was
[00:06:52.060 --> 00:06:56.300]   the one that basically laid out what everybody's doing, how this system works.
[00:06:56.300 --> 00:06:59.980]   And the system has three steps.
[00:06:59.980 --> 00:07:03.020]   Step one is language model training.
[00:07:03.020 --> 00:07:05.140]   But you'll see this is actually from the paper.
[00:07:05.140 --> 00:07:07.900]   We actually described it as pre-training.
[00:07:07.900 --> 00:07:12.820]   Now what language model pre-training does is this is the thing which predicts the next
[00:07:12.820 --> 00:07:14.180]   word of a sentence.
[00:07:14.180 --> 00:07:19.780]   And so in the original ULMfit paper, so the algorithm I developed in 2017, then Sebastian
[00:07:19.780 --> 00:07:24.800]   Ruder and I wrote it up in 2018, early 2018.
[00:07:24.800 --> 00:07:28.940]   What I originally did was I trained this language model on Wikipedia.
[00:07:28.940 --> 00:07:35.980]   Now what that meant is I took a neural network, and a neural network is just a function.
[00:07:35.980 --> 00:07:39.420]   If you don't know what it is, it's just a mathematical function that's extremely flexible
[00:07:39.420 --> 00:07:41.100]   and it's got lots and lots of parameters.
[00:07:41.100 --> 00:07:47.140]   And initially it can't do anything, but using stochastic gradient descent, or SGD, you can
[00:07:47.140 --> 00:07:50.800]   teach it to do almost anything if you give it examples.
[00:07:50.800 --> 00:07:54.260]   And so I gave it lots of examples of sentences from Wikipedia.
[00:07:54.260 --> 00:07:57.660]   So for example, from the Wikipedia article for the birds.
[00:07:57.660 --> 00:08:04.940]   The birds is a 1963 American natural horror thriller film produced and directed by Alfred,
[00:08:04.940 --> 00:08:06.540]   and then it would stop.
[00:08:06.540 --> 00:08:10.420]   And so then the model would have to guess what the next word is.
[00:08:10.420 --> 00:08:14.060]   And if it guessed Hitchcock, it would be rewarded.
[00:08:14.060 --> 00:08:17.520]   And if it guessed something else, it would be penalized.
[00:08:17.520 --> 00:08:20.860]   And effectively, basically it's trying to maximize those rewards.
[00:08:20.860 --> 00:08:25.460]   It's trying to find a set of weights for this function that makes it more likely that it
[00:08:25.460 --> 00:08:26.900]   would predict Hitchcock.
[00:08:26.900 --> 00:08:32.380]   And then later on in this article, it reads from Wikipedia, "Annie previously dated Mitch,
[00:08:32.380 --> 00:08:39.020]   but ended it due to Mitch's cold overbearing mother, Lydia, who dislikes any woman in Mitch's."
[00:08:39.020 --> 00:08:44.420]   Now you can see that filling this in actually requires being pretty thoughtful, because
[00:08:44.420 --> 00:08:48.180]   there's a bunch of things that kind of logically could go there.
[00:08:48.180 --> 00:08:57.140]   Like a woman could be in Mitch's closet, could be in Mitch's house.
[00:08:57.140 --> 00:09:01.260]   And so, you know, you could probably guess in the Wikipedia article describing the plot
[00:09:01.260 --> 00:09:05.500]   of the birds, it's actually any woman in Mitch's life.
[00:09:05.500 --> 00:09:13.580]   Now to do a good job of solving this problem as well as possible, of guessing the next
[00:09:13.580 --> 00:09:22.420]   word of sentences, the neural network is going to have to learn a lot of stuff about the
[00:09:22.420 --> 00:09:23.420]   world.
[00:09:23.420 --> 00:09:28.580]   It's going to learn that there are things called objects, that there's a thing called
[00:09:28.580 --> 00:09:34.940]   time, that objects react to each other over time, that there are things called movies,
[00:09:34.940 --> 00:09:39.340]   that movies have directors, that there are people, that people have names, and so forth.
[00:09:39.340 --> 00:09:46.700]   And that a movie director is Alfred Hitchcock, and he directed horror films, and so on and
[00:09:46.700 --> 00:09:47.980]   so forth.
[00:09:47.980 --> 00:09:51.980]   It's going to have to learn an extraordinary amount if it's going to do a really good job
[00:09:51.980 --> 00:09:54.860]   of predicting the next word of sentences.
[00:09:54.860 --> 00:10:00.700]   Now these neural networks specifically are deep neural networks, this is deep learning.
[00:10:00.700 --> 00:10:06.660]   And in these deep neural networks, which have, when I created this, I think it had like 100
[00:10:06.660 --> 00:10:13.500]   million parameters, nowadays they have billions of parameters.
[00:10:13.500 --> 00:10:20.820]   It's got the ability to create a rich hierarchy of abstractions and representations which
[00:10:20.820 --> 00:10:23.780]   it can build on.
[00:10:23.780 --> 00:10:31.820]   And so this is really the key idea behind neural networks and language models, is that
[00:10:31.820 --> 00:10:36.780]   if it's going to do a good job of being able to predict the next word of any sentence in
[00:10:36.780 --> 00:10:41.220]   any situation, it's going to have to know an awful lot about the world, it's going to
[00:10:41.220 --> 00:10:47.500]   have to know about how to solve math questions, or figure out the next move in a chess game,
[00:10:47.500 --> 00:10:52.940]   or recognize poetry, and so on and so forth.
[00:10:52.940 --> 00:10:59.540]   Now nobody said it's going to do a good job of that, so it's a lot of work to find, to
[00:10:59.540 --> 00:11:02.180]   create and train a model that is good at that.
[00:11:02.180 --> 00:11:09.380]   But if you can create one that's good at that, it's going to have a lot of capabilities internally
[00:11:09.380 --> 00:11:13.180]   that it would have to be drawing on to be able to do this effectively.
[00:11:13.180 --> 00:11:19.180]   So the key idea here for me is that this is a form of compression.
[00:11:19.180 --> 00:11:24.140]   And this idea of the relationship between compression and intelligence goes back many
[00:11:24.140 --> 00:11:26.380]   many decades.
[00:11:26.380 --> 00:11:33.580]   And the basic idea is that, yeah, if you can guess what words are coming up next, then
[00:11:33.580 --> 00:11:41.420]   effectively you're compressing all that information down into a neural network.
[00:11:41.420 --> 00:11:44.420]   Now I said this is not useful of itself.
[00:11:44.420 --> 00:11:45.420]   Why do we do it?
[00:11:45.420 --> 00:11:49.600]   Well we do it because we want to pull out those capabilities.
[00:11:49.600 --> 00:11:53.420]   And the way we pull out those capabilities is we take two more steps.
[00:11:53.420 --> 00:11:57.300]   The second step is we do something called language model fine tuning.
[00:11:57.300 --> 00:12:03.540]   And in language model fine tuning, we are no longer just giving it all of Wikipedia,
[00:12:03.540 --> 00:12:09.900]   or nowadays we don't just give it all of Wikipedia, but in fact a large chunk of the internet
[00:12:09.900 --> 00:12:12.360]   is fed to pre-training these models.
[00:12:12.360 --> 00:12:19.380]   In the fine tuning stage, we feed it a set of documents a lot closer to the final task
[00:12:19.380 --> 00:12:22.340]   that we want the model to do.
[00:12:22.340 --> 00:12:23.700]   But it's still the same basic idea.
[00:12:23.700 --> 00:12:29.620]   It's still trying to predict the next word of a sentence.
[00:12:29.620 --> 00:12:32.900]   After that, we then do a final classifier fine tuning.
[00:12:32.900 --> 00:12:38.460]   And in the classifier fine tuning, this is the kind of end task we're trying to get it
[00:12:38.460 --> 00:12:39.580]   to do.
[00:12:39.580 --> 00:12:45.220]   Now, nowadays, these two steps are very specific approaches are taken.
[00:12:45.220 --> 00:12:50.820]   For the step two, the step B, the language model fine tuning, people nowadays do a particular
[00:12:50.820 --> 00:12:53.140]   kind called instruction tuning.
[00:12:53.140 --> 00:13:00.380]   The idea is that the task we want most of the time to achieve is solve problems, answer
[00:13:00.380 --> 00:13:01.380]   questions.
[00:13:01.380 --> 00:13:06.580]   And so in the instruction tuning phase, we use data sets like this one.
[00:13:06.580 --> 00:13:12.740]   This is a great data set called OpenOrca, created by a fantastic open source group.
[00:13:12.740 --> 00:13:18.540]   And it's built on top of something called the Flan collection.
[00:13:18.540 --> 00:13:24.580]   And you can see that basically there's all kinds of different questions in here.
[00:13:24.580 --> 00:13:31.600]   So there's four gigabytes of questions, and context, and so forth.
[00:13:31.600 --> 00:13:40.420]   And each one generally has a question, or an instruction, or a request, and then a response.
[00:13:40.420 --> 00:13:44.300]   Here are some examples of instructions.
[00:13:44.300 --> 00:13:47.560]   I think this is from the Flan data set, if I remember correctly.
[00:13:47.560 --> 00:13:52.620]   So for instance, it could be, "Does the sentence 'In the Iron Age' answer the question, 'The
[00:13:52.620 --> 00:13:56.620]   period of time from 1200 to 1000 BCE is known as what?'
[00:13:56.620 --> 00:13:59.260]   Choice is 1, yes or no."
[00:13:59.260 --> 00:14:07.340]   And then the language model is meant to write 1 or 2 as appropriate for yes or no.
[00:14:07.340 --> 00:14:12.300]   Or it could be things about, I think this is from a music video, "Who is the girl in
[00:14:12.300 --> 00:14:13.300]   more than you know?"
[00:14:13.300 --> 00:14:18.060]   Answer, and then it would have to write the correct name of the, I can't remember, model,
[00:14:18.060 --> 00:14:22.100]   or dancer, or whatever, from that music video.
[00:14:22.100 --> 00:14:23.100]   And so forth.
[00:14:23.100 --> 00:14:26.240]   So it's still doing language modeling.
[00:14:26.240 --> 00:14:29.780]   So fine-tuning and pre-training are kind of the same thing.
[00:14:29.780 --> 00:14:36.120]   But this is more targeted now, not just to be able to fill in the missing parts of any
[00:14:36.120 --> 00:14:44.000]   document from the internet, but to fill in the words necessary to answer questions, to
[00:14:44.000 --> 00:14:45.640]   do useful things.
[00:14:45.640 --> 00:14:49.720]   Okay, so that's instruction tuning.
[00:14:49.720 --> 00:14:53.800]   And then step 3, which is the classifier fine-tuning.
[00:14:53.800 --> 00:14:58.200]   Nowadays there's generally various approaches, such as reinforcement learning from human
[00:14:58.200 --> 00:15:09.880]   feedback and others, which are basically giving humans, or sometimes more advanced models,
[00:15:09.880 --> 00:15:14.880]   multiple answers to a question such as, here are some from a reinforcement learning from
[00:15:14.880 --> 00:15:19.000]   human feedback paper, I can't remember which one I got it from, "List 5 ideas for how to
[00:15:19.000 --> 00:15:21.680]   regain enthusiasm for my career."
[00:15:21.680 --> 00:15:27.120]   And so the model will spit out 2 possible answers, or it will have a less good model
[00:15:27.120 --> 00:15:34.000]   and a more good model, and then a human or a better model will pick which is best.
[00:15:34.000 --> 00:15:38.160]   And so that's used for the final fine-tuning stage.
[00:15:38.160 --> 00:15:48.360]   So all of that is to say, although you can download pure language models from the internet,
[00:15:48.360 --> 00:15:54.520]   they're not generally that useful on their own until you've fine-tuned them.
[00:15:54.520 --> 00:15:58.320]   Now you don't necessarily need step C nowadays, actually people are discovering that maybe
[00:15:58.320 --> 00:16:00.240]   just step B might be enough.
[00:16:00.240 --> 00:16:02.960]   It's still a bit controversial.
[00:16:02.960 --> 00:16:09.440]   Okay, so when we talk about a language model, we could be talking about something that's
[00:16:09.440 --> 00:16:13.960]   just been pre-trained, something that's been fine-tuned, or something that's gone through
[00:16:13.960 --> 00:16:16.000]   something like RLHF.
[00:16:16.000 --> 00:16:23.140]   All of those things are generally described nowadays as language models.
[00:16:23.140 --> 00:16:31.000]   So my view is that if you are going to be good at language modeling in any way, then
[00:16:31.000 --> 00:16:35.680]   you need to start by being a really effective user of language models.
[00:16:35.680 --> 00:16:38.960]   And to be a really effective user of language models, you've got to use the best one that
[00:16:38.960 --> 00:16:39.960]   there is.
[00:16:39.960 --> 00:16:48.600]   And currently, so what are we up to, September 2023, the best one is by far GPT-4.
[00:16:48.600 --> 00:16:53.960]   This might change sometime in the not-too-distant future, but this is right now, GPT-4 is the
[00:16:53.960 --> 00:16:56.520]   recommendation, strong, strong recommendation.
[00:16:56.520 --> 00:17:04.200]   Now you can use GPT-4 by paying 20 bucks a month to open AI, and then you can use it
[00:17:04.200 --> 00:17:05.200]   a whole lot.
[00:17:05.200 --> 00:17:11.000]   It's very hard to run out of credits, I find.
[00:17:11.000 --> 00:17:14.360]   Now what can GPT-2?
[00:17:14.360 --> 00:17:20.040]   It's interesting and instructive, in my opinion, to start with the very common views you see
[00:17:20.040 --> 00:17:23.640]   on the internet, or even in academia, about what it can't do.
[00:17:23.640 --> 00:17:30.400]   So for example, there was this paper you might have seen, "GPT-4 can't reason," which describes
[00:17:30.400 --> 00:17:38.800]   a number of empirical analysis done of 25 diverse reasoning problems, and found that
[00:17:38.800 --> 00:17:44.880]   it was not able to solve them, it's utterly incapable of reasoning.
[00:17:44.880 --> 00:17:50.880]   So I always find you've got to be a bit careful about reading stuff like this, because I just
[00:17:50.880 --> 00:18:00.520]   took the first three that I came across in that paper, and I gave them to GPT-4.
[00:18:00.520 --> 00:18:09.360]   And by the way, something very useful in GPT-4 is you can click on the share button, and
[00:18:09.360 --> 00:18:12.920]   you'll get something that looks like this, and this is really handy.
[00:18:12.920 --> 00:18:18.280]   So here's an example of something from the paper that said GPT-4 can't do this.
[00:18:18.280 --> 00:18:22.000]   "Mabel's heart rate at 9am was 75 beats per minute.
[00:18:22.000 --> 00:18:25.240]   Her blood pressure at 7pm was 120/80.
[00:18:25.240 --> 00:18:26.840]   She died at 11pm.
[00:18:26.840 --> 00:18:27.840]   Was she alive at noon?"
[00:18:27.840 --> 00:18:32.800]   Of course, she's a human, we know obviously she must be.
[00:18:32.800 --> 00:18:38.560]   And GPT-4 says, "Hmm, this appears to be a riddle, not a real inquiry into medical conditions."
[00:18:38.560 --> 00:18:47.440]   Here's a summary of the information, and yeah, it sounds like Mabel was alive at noon.
[00:18:47.440 --> 00:18:48.720]   So it's correct.
[00:18:48.720 --> 00:18:53.960]   This was the second one I tried from the paper that says GPT-4 can't do this, and I found
[00:18:53.960 --> 00:19:01.440]   actually GPT-4 can do this, and it said that GPT-4 can't do this, and I found GPT-4 can
[00:19:01.440 --> 00:19:02.440]   do this.
[00:19:02.440 --> 00:19:09.880]   Now, I mention this to say GPT-4 is probably a lot better than you would expect if you've
[00:19:09.880 --> 00:19:17.440]   read all this stuff on the internet about all the dumb things that it does.
[00:19:17.440 --> 00:19:23.040]   Almost every time I see on the internet saying something that GPT-4 can't do, I check it
[00:19:23.040 --> 00:19:24.080]   and it turns out it does.
[00:19:24.080 --> 00:19:25.600]   This one was just last week.
[00:19:25.600 --> 00:19:28.760]   "Sally, a girl, has three brothers.
[00:19:28.760 --> 00:19:30.600]   Each brother has two sisters.
[00:19:30.600 --> 00:19:33.600]   How many sisters does Sally have?"
[00:19:33.600 --> 00:19:37.260]   So have a think about it.
[00:19:37.260 --> 00:19:43.640]   And so GPT-4 says, "Okay, Sally is counted as one sister by each of her brothers.
[00:19:43.640 --> 00:19:47.800]   If each brother has two sisters, that means there's another sister in the picture apart
[00:19:47.800 --> 00:19:48.880]   from Sally.
[00:19:48.880 --> 00:19:50.240]   So Sally has one sister."
[00:19:50.240 --> 00:19:57.200]   Yep, correct.
[00:19:57.200 --> 00:20:01.320]   And then this one I got sort of like three or four days ago.
[00:20:01.320 --> 00:20:07.120]   This is a common view that language models can't track things like this.
[00:20:07.120 --> 00:20:08.120]   Here's the riddle.
[00:20:08.120 --> 00:20:09.120]   "I'm in my house.
[00:20:09.120 --> 00:20:11.680]   On top of my chair in the living room is a coffee cup.
[00:20:11.680 --> 00:20:13.540]   Inside the coffee cup is a thimble.
[00:20:13.540 --> 00:20:15.520]   Inside the thimble is a diamond.
[00:20:15.520 --> 00:20:17.000]   I move the chair to the bedroom.
[00:20:17.000 --> 00:20:18.200]   I put the coffee cup on the bed.
[00:20:18.200 --> 00:20:19.960]   I turn the cup upside down.
[00:20:19.960 --> 00:20:21.440]   Then I return it upside up.
[00:20:21.440 --> 00:20:23.400]   Place the coffee cup on the counter in the kitchen.
[00:20:23.400 --> 00:20:25.320]   Where's my diamond?"
[00:20:25.320 --> 00:20:28.480]   And so GPT-4 says, "Yep, okay.
[00:20:28.480 --> 00:20:30.040]   You turned it upside down.
[00:20:30.040 --> 00:20:32.520]   So probably the diamond fell out.
[00:20:32.520 --> 00:20:37.480]   So therefore the diamond is in the bedroom where it fell out."
[00:20:37.480 --> 00:20:38.480]   Again, correct.
[00:20:38.480 --> 00:20:44.720]   Why is it that people are claiming that GPT-4 can't do these things?
[00:20:44.720 --> 00:20:45.720]   And it can.
[00:20:45.720 --> 00:20:52.800]   The reason is because I think on the whole they are not aware of how GPT-4 was trained.
[00:20:52.800 --> 00:20:59.720]   GPT-4 was not trained at any point to give correct answers.
[00:20:59.720 --> 00:21:06.320]   GPT-4 was trained initially to give most likely next words.
[00:21:06.320 --> 00:21:11.320]   And there's an awful lot of stuff on the internet where documents are not describing things
[00:21:11.320 --> 00:21:12.320]   that are true.
[00:21:12.320 --> 00:21:13.720]   They could be fiction.
[00:21:13.720 --> 00:21:14.720]   They could be jokes.
[00:21:14.720 --> 00:21:18.660]   They could be just stupid people saying dumb stuff.
[00:21:18.660 --> 00:21:23.240]   So this first stage does not necessarily give you correct answers.
[00:21:23.240 --> 00:21:31.720]   The second stage with the instruction tuning, also, it's trying to give correct answers.
[00:21:31.720 --> 00:21:36.300]   But part of the problem is that then in the stage where you start asking people which
[00:21:36.300 --> 00:21:44.840]   answer do they like better, people tended to say in these things that they prefer more
[00:21:44.840 --> 00:21:47.280]   confident answers.
[00:21:47.280 --> 00:21:53.220]   And they often were not people who were trained well enough to recognize wrong answers.
[00:21:53.220 --> 00:21:58.360]   So there's lots of reasons that the, that the, you know, SGD weight updates from this
[00:21:58.360 --> 00:22:06.720]   process for stuff like GPT-4 don't particularly or don't entirely reward correct answers.
[00:22:06.720 --> 00:22:11.020]   But you can help it want to give you correct answers.
[00:22:11.020 --> 00:22:17.840]   If you think about the LM pre-training, what are the kinds of things in a document that
[00:22:17.840 --> 00:22:22.620]   would suggest, oh, this is going to be high quality information.
[00:22:22.620 --> 00:22:30.160]   And so you can actually prime GPT-4 to give you high quality information by giving it
[00:22:30.160 --> 00:22:33.120]   custom instructions.
[00:22:33.120 --> 00:22:40.800]   And what this does is this is basically text that is prepended to all of your queries.
[00:22:40.800 --> 00:22:44.480]   And so you say, like, oh, you're brilliant at reasoning.
[00:22:44.480 --> 00:22:50.920]   So like, okay, that's obviously you have to prime it to give good answers.
[00:22:50.920 --> 00:23:00.320]   And then try to work against the fact that the RLHF folks preferred confidence.
[00:23:00.320 --> 00:23:01.320]   Just tell it.
[00:23:01.320 --> 00:23:05.720]   No, tell me if there might not be a correct answer.
[00:23:05.720 --> 00:23:12.680]   Also the way that the text is generated is it literally generates the next word.
[00:23:12.680 --> 00:23:17.400]   And then it puts all that whole lot back into the model and generates the next next word,
[00:23:17.400 --> 00:23:22.200]   puts that all back in the model, generates the next next next word, and so forth.
[00:23:22.200 --> 00:23:25.880]   That means the more words it generates, the more computation it can do.
[00:23:25.880 --> 00:23:28.800]   And so I literally, I tell it that.
[00:23:28.800 --> 00:23:34.880]   And so I say, first, spend a few sentences explaining background context, etc.
[00:23:34.880 --> 00:23:48.240]   So this custom instruction allows it to solve more challenging problems.
[00:23:48.240 --> 00:23:52.200]   And you can see the difference.
[00:23:52.200 --> 00:23:56.320]   Here's what it looks like, for example, if I say, how do I get a count of rows grouped
[00:23:56.320 --> 00:23:58.700]   by value in pandas?
[00:23:58.700 --> 00:24:03.200]   And it just gives me a whole lot of information, which is actually it thinking.
[00:24:03.200 --> 00:24:06.640]   So I just skip over it, and then it gives me the answer.
[00:24:06.640 --> 00:24:16.400]   And actually in my custom instructions, I actually say, if the request begins with VV,
[00:24:16.400 --> 00:24:19.140]   actually make it as concise as possible.
[00:24:19.140 --> 00:24:22.100]   And so it kind of goes into brief mode.
[00:24:22.100 --> 00:24:23.100]   And here it's brief mode.
[00:24:23.100 --> 00:24:24.100]   How do I get the group?
[00:24:24.100 --> 00:24:26.800]   This is the same thing, but with VV at the start.
[00:24:26.800 --> 00:24:28.640]   And it just spits it out.
[00:24:28.640 --> 00:24:30.560]   Now in this case, it's a really simple question.
[00:24:30.560 --> 00:24:33.000]   So I didn't need time to think.
[00:24:33.000 --> 00:24:41.120]   So hopefully that gives you a sense of how to get language models to give good answers.
[00:24:41.120 --> 00:24:42.760]   You have to help them.
[00:24:42.760 --> 00:24:47.880]   And if it's not working, it might be user error, basically.
[00:24:47.880 --> 00:24:54.320]   But having said that, there's plenty of stuff that language models like GPT-4 can't do.
[00:24:54.320 --> 00:24:59.400]   One thing to think carefully about is, does it know about itself?
[00:24:59.400 --> 00:25:00.600]   Can you ask it?
[00:25:00.600 --> 00:25:02.780]   What is your context length?
[00:25:02.780 --> 00:25:04.680]   How were you trained?
[00:25:04.680 --> 00:25:09.760]   What transformer architecture are you based on?
[00:25:09.760 --> 00:25:15.680]   At any one of these stages, did it have the opportunity to learn any of those things?
[00:25:15.680 --> 00:25:17.560]   Well obviously not at the pre-training stage.
[00:25:17.560 --> 00:25:26.000]   Nothing on the internet existed during GPT-4's training saying how GPT-4 was trained.
[00:25:26.000 --> 00:25:27.920]   Probably ditto in the instruction tuning.
[00:25:27.920 --> 00:25:29.440]   Probably ditto in the RLHF.
[00:25:29.440 --> 00:25:35.480]   So in general, you can't ask, for example, a language model about itself.
[00:25:35.480 --> 00:25:40.680]   Now again, because of the RLHF, it'll want to make you happy by giving you opinionated
[00:25:40.680 --> 00:25:41.780]   answers.
[00:25:41.780 --> 00:25:48.340]   So it'll just spit out the most likely thing it thinks with great confidence.
[00:25:48.340 --> 00:25:50.360]   This is just a general kind of hallucination, right?
[00:25:50.360 --> 00:25:56.200]   So hallucinations is just this idea that the language model wants to complete the sentence
[00:25:56.200 --> 00:26:02.640]   and it wants to do it in an opinionated way that's likely to make people happy.
[00:26:02.640 --> 00:26:04.360]   It doesn't know anything about URLs.
[00:26:04.360 --> 00:26:07.560]   It really hasn't seen many at all.
[00:26:07.560 --> 00:26:11.720]   I think a lot of them, if not all of them, pretty much were stripped out.
[00:26:11.720 --> 00:26:16.720]   So if you ask it anything about like what's at this web page, again, it'll generally just
[00:26:16.720 --> 00:26:19.240]   make it up.
[00:26:19.240 --> 00:26:24.280]   And it doesn't know, at least GPT-4 doesn't know anything after September 2021.
[00:26:24.280 --> 00:26:32.280]   Because the information it was pre-trained on was from that time period, September 2021
[00:26:32.280 --> 00:26:35.680]   and before, called the knowledge cutoff.
[00:26:35.680 --> 00:26:39.120]   So here's some things it can't do.
[00:26:39.120 --> 00:26:45.700]   Steve Newman sent me this good example of something that it can't do.
[00:26:45.700 --> 00:26:47.120]   Here is a logic puzzle.
[00:26:47.120 --> 00:26:51.400]   I need to carry a cabbage, a goat and a wolf across a river.
[00:26:51.400 --> 00:26:53.720]   I can only carry one item at a time.
[00:26:53.720 --> 00:26:55.920]   I can't leave the goat with a cabbage.
[00:26:55.920 --> 00:26:58.640]   I can't leave the cabbage with the wolf.
[00:26:58.640 --> 00:27:01.280]   How do I get everything across to the other side?
[00:27:01.280 --> 00:27:09.160]   Now the problem is this looks a lot like something called the classic river crossing puzzle.
[00:27:09.160 --> 00:27:16.380]   So classic in fact that it has a whole Wikipedia page about it.
[00:27:16.380 --> 00:27:25.160]   And in the classic puzzle, the wolf would eat the goat or the goat would eat the cabbage.
[00:27:25.160 --> 00:27:34.840]   Now in Steve's version, he changed it.
[00:27:34.840 --> 00:27:39.000]   The goat would eat the cabbage and the wolf would eat the cabbage, but the wolf won't
[00:27:39.000 --> 00:27:42.060]   eat the goat.
[00:27:42.060 --> 00:27:43.060]   So what happens?
[00:27:43.060 --> 00:27:49.400]   Well very interestingly, GPT-4 here is entirely overwhelmed by the language model training.
[00:27:49.400 --> 00:27:53.620]   It's seen this puzzle so many times, it knows what word comes next.
[00:27:53.620 --> 00:27:57.600]   So it says, oh yeah, I take the goat across the road, across the river and leave it on
[00:27:57.600 --> 00:27:58.800]   the other side.
[00:27:58.800 --> 00:28:02.680]   Leaving the wolf with a cabbage, but we're just told you can't leave the wolf with a
[00:28:02.680 --> 00:28:04.460]   cabbage.
[00:28:04.460 --> 00:28:07.180]   So it gets it wrong.
[00:28:07.180 --> 00:28:12.920]   Now the thing is though, you can encourage GPT-4 or any of these language models to try
[00:28:12.920 --> 00:28:13.920]   again.
[00:28:13.920 --> 00:28:18.560]   So during the instruction tuning in RLHF, they're actually fine-tuned with multi-stage
[00:28:18.560 --> 00:28:20.040]   conversations.
[00:28:20.040 --> 00:28:22.640]   So you can give it a multi-stage conversation.
[00:28:22.640 --> 00:28:24.780]   Repeat back to me the constraints I listed.
[00:28:24.780 --> 00:28:26.500]   What happened after step one?
[00:28:26.500 --> 00:28:27.500]   Is a constraint violated?
[00:28:27.500 --> 00:28:32.440]   Oh yeah, yeah, yeah, I made a mistake.
[00:28:32.440 --> 00:28:33.440]   Okay.
[00:28:33.440 --> 00:28:36.940]   So a new attempt, instead of taking the goat across the river and leaving it on the other
[00:28:36.940 --> 00:28:41.820]   side, is I'll take the goat across the river and leave it on the other side.
[00:28:41.820 --> 00:28:44.820]   It's done the same thing.
[00:28:44.820 --> 00:28:47.660]   Oh yeah, I did do the same thing.
[00:28:47.660 --> 00:28:49.940]   Okay, I'll take the wolf across.
[00:28:49.940 --> 00:28:51.380]   Well now the goat's with a cabbage.
[00:28:51.380 --> 00:28:52.380]   That still doesn't work.
[00:28:52.380 --> 00:28:57.100]   Oh yeah, that didn't work either.
[00:28:57.100 --> 00:28:58.680]   Sorry about that.
[00:28:58.680 --> 00:29:01.020]   Instead of taking the goat across the other side, I'll take the goat across the other
[00:29:01.020 --> 00:29:02.020]   side.
[00:29:02.020 --> 00:29:03.820]   Okay, what's going on here?
[00:29:03.820 --> 00:29:06.100]   This is terrible.
[00:29:06.100 --> 00:29:14.340]   Well one of the problems here is that not only is on the internet it's so common to
[00:29:14.340 --> 00:29:19.900]   see this particular goat puzzle, that it's so confident it knows what the next word is.
[00:29:19.900 --> 00:29:26.300]   Also on the internet, when you see stuff which is stupid on a web page, it's really likely
[00:29:26.300 --> 00:29:31.060]   to be followed up with more stuff that is stupid.
[00:29:31.060 --> 00:29:38.860]   Once GPT-4 starts being wrong, it tends to be more and more wrong.
[00:29:38.860 --> 00:29:44.540]   It's very hard to turn it around to start making it be right.
[00:29:44.540 --> 00:29:57.380]   So you actually have to go back, and there's actually an edit button on these chats.
[00:29:57.380 --> 00:30:01.500]   And so what you generally want to do is, if it's made a mistake, is don't say "oh here's
[00:30:01.500 --> 00:30:08.100]   more information to help you fix it", but instead go back and click the edit and change
[00:30:08.100 --> 00:30:16.260]   it here.
[00:30:16.260 --> 00:30:21.060]   And so this time it's not going to get confused.
[00:30:21.060 --> 00:30:27.020]   So in this case actually fixing Steve's example takes quite a lot of effort, but I think I
[00:30:27.020 --> 00:30:29.500]   managed to get it to work eventually.
[00:30:29.500 --> 00:30:33.700]   And I actually said "oh sometimes people read things too quickly, they don't notice things,
[00:30:33.700 --> 00:30:38.460]   it can trick them up, then they apply some pattern, get the wrong answer.
[00:30:38.460 --> 00:30:42.080]   You do the same thing by the way, so I'm going to trick you.
[00:30:42.080 --> 00:30:46.660]   So before you're about to get tricked, make sure you don't get tricked.
[00:30:46.660 --> 00:30:48.740]   Here's the tricky puzzle.
[00:30:48.740 --> 00:30:54.780]   And then also with my custom instructions it takes time discussing it.
[00:30:54.780 --> 00:30:58.260]   And this time it gets it correct, it takes the cabbage across first.
[00:30:58.260 --> 00:31:03.300]   So it took a lot of effort to get to a point where it could actually solve this.
[00:31:03.300 --> 00:31:10.500]   Because yeah, when it's, you know, for things where it's been primed to answer a certain
[00:31:10.500 --> 00:31:15.980]   way again and again and again, it's very hard for it to not do that.
[00:31:15.980 --> 00:31:24.780]   Okay, now something else super helpful that you can use is what they call advanced data
[00:31:24.780 --> 00:31:26.580]   analysis.
[00:31:26.580 --> 00:31:32.020]   In advanced data analysis you can ask it to basically write code for you.
[00:31:32.020 --> 00:31:34.780]   And we're going to look at how to implement this from scratch ourself quite soon.
[00:31:34.780 --> 00:31:37.320]   But first of all let's learn how to use it.
[00:31:37.320 --> 00:31:44.300]   So I was trying to build something that split into markdown headings, a document on third
[00:31:44.300 --> 00:31:49.860]   level markdown headings, so that's three hashes at the start of a line.
[00:31:49.860 --> 00:31:54.860]   And I was doing it on the whole of Wikipedia, so using regular expressions was really slow.
[00:31:54.860 --> 00:31:57.900]   So I said, "Oh, I want to speed this up."
[00:31:57.900 --> 00:32:00.620]   And it said, "Okay, here's some code."
[00:32:00.620 --> 00:32:05.900]   Which is great, because then I can say, "Okay, test it and include edge cases."
[00:32:05.900 --> 00:32:17.060]   And so it then puts in the code, creates the edge cases, tests it, and says, "Yep, it's
[00:32:17.060 --> 00:32:18.060]   working."
[00:32:18.060 --> 00:32:20.860]   However, I discovered it's not.
[00:32:20.860 --> 00:32:24.740]   I noticed it's actually removing the carriage return at the end of each sentence.
[00:32:24.740 --> 00:32:28.980]   So I said, "Oh, fix that and update your tests."
[00:32:28.980 --> 00:32:31.640]   So it said, "Okay."
[00:32:31.640 --> 00:32:37.660]   So now it's changed the test, updated the test cases, it's run them, and oh, it's not
[00:32:37.660 --> 00:32:38.900]   working.
[00:32:38.900 --> 00:32:42.820]   So it says, "Oh yeah, fix the issue in the test cases."
[00:32:42.820 --> 00:32:45.900]   Nope, it didn't work.
[00:32:45.900 --> 00:32:52.980]   And you can see it's quite clever the way it's trying to fix it by looking at the results.
[00:32:52.980 --> 00:32:56.580]   But as you can see, it's not.
[00:32:56.580 --> 00:33:00.220]   Every one of these is another attempt, another attempt, another attempt, until eventually
[00:33:00.220 --> 00:33:01.940]   I gave up waiting.
[00:33:01.940 --> 00:33:05.180]   It's so funny, each time it's like, "Debugging again.
[00:33:05.180 --> 00:33:08.420]   Okay, this time I've got to handle it properly."
[00:33:08.420 --> 00:33:12.060]   And I gave up at the point where it's like, "Oh, one more attempt."
[00:33:12.060 --> 00:33:15.700]   So it didn't solve it, interestingly enough.
[00:33:15.700 --> 00:33:24.860]   And you know, again, there's some limits to the amount of logic that it can do.
[00:33:24.860 --> 00:33:29.040]   This is really a very simple question I asked it to do for me.
[00:33:29.040 --> 00:33:35.580]   And so hopefully you can see, you can't expect even GPT-4 code interpreter, or advanced data
[00:33:35.580 --> 00:33:41.620]   analysis as it's now called, to make it so you don't have to write code anymore.
[00:33:41.620 --> 00:33:47.500]   You know, it's not a substitute for having programmers.
[00:33:47.500 --> 00:33:53.940]   So but it can, you know, it can often do a lot, as I'll show you in a moment.
[00:33:53.940 --> 00:34:00.820]   So for example, actually OCR, like this is something I thought was really cool.
[00:34:00.820 --> 00:34:03.900]   You can just paste and, sorry, paste to upload.
[00:34:03.900 --> 00:34:10.300]   So GPT-4, you can upload an image.
[00:34:10.300 --> 00:34:16.060]   Advanced data analysis, yeah, you can upload an image here.
[00:34:16.060 --> 00:34:20.500]   And then I wanted to basically grab some text out of an image.
[00:34:20.500 --> 00:34:24.780]   Somebody had got a screenshot of their screen, and I wanted to, which was something saying,
[00:34:24.780 --> 00:34:27.300]   "Oh, this language model can't do this."
[00:34:27.300 --> 00:34:28.540]   And I wanted to try it as well.
[00:34:28.540 --> 00:34:32.180]   So rather than retyping it, I just uploaded that image, my screenshot, and said, "Can
[00:34:32.180 --> 00:34:34.900]   you extract the text from this image?"
[00:34:34.900 --> 00:34:36.300]   And it said, "Oh yeah, I could do that.
[00:34:36.300 --> 00:34:38.540]   I could use OCR."
[00:34:38.540 --> 00:34:45.660]   And like, so it literally wrote an OCR script, and there it is.
[00:34:45.660 --> 00:34:46.920]   Just took a few seconds.
[00:34:46.920 --> 00:34:53.680]   So the difference here is it didn't really require to think of much logic.
[00:34:53.680 --> 00:34:59.540]   It could just use a very, very familiar pattern that it would have seen many times.
[00:34:59.540 --> 00:35:04.220]   So this is generally where I find language models excel, is where it doesn't have to
[00:35:04.220 --> 00:35:06.300]   think too far outside the box.
[00:35:06.300 --> 00:35:11.580]   I mean, it's great on kind of creativity tasks, but for like reasoning and logic tasks that
[00:35:11.580 --> 00:35:14.880]   are outside the box, I find it not great.
[00:35:14.880 --> 00:35:21.500]   But yeah, it's great at doing code for a whole wide variety of different libraries and languages.
[00:35:21.500 --> 00:35:28.440]   Having said that, by the way, Google also has a language model called BARD.
[00:35:28.440 --> 00:35:32.640]   It's way less good than GPT-4 most of the time, but there is a nice thing that you can
[00:35:32.640 --> 00:35:37.240]   literally paste an image straight into the prompt.
[00:35:37.240 --> 00:35:41.160]   And I just typed "OCR this," and it didn't even have to go through Code Interpreter or
[00:35:41.160 --> 00:35:42.160]   whatever.
[00:35:42.160 --> 00:35:43.160]   It just said, "Oh, sure.
[00:35:43.160 --> 00:35:44.160]   I've done it."
[00:35:44.160 --> 00:35:47.240]   And there's the result of the OCR.
[00:35:47.240 --> 00:35:51.040]   And then it even commented on what it just OCR'd, which I thought was cute.
[00:35:51.040 --> 00:35:57.840]   And oh, even more interestingly, it even figured out where the OCR text came from and gave
[00:35:57.840 --> 00:36:00.400]   me a link to it.
[00:36:00.400 --> 00:36:03.080]   So I thought that was pretty cool.
[00:36:03.080 --> 00:36:04.080]   Okay.
[00:36:04.080 --> 00:36:07.760]   So there's an example of it doing well.
[00:36:07.760 --> 00:36:10.840]   I'll show you one for this talk I found really helpful.
[00:36:10.840 --> 00:36:18.240]   I wanted to show you guys how much it costs to use the OpenAI API.
[00:36:18.240 --> 00:36:23.800]   But unfortunately when I went to the OpenAI web page, it was, like, all over the place.
[00:36:23.800 --> 00:36:29.960]   The pricing information was on all separate tables, and it was kind of a bit of a mess.
[00:36:29.960 --> 00:36:37.600]   So I wanted to create a table with all of the information combined like this.
[00:36:37.600 --> 00:36:42.200]   And here's how I did it.
[00:36:42.200 --> 00:36:45.520]   I went to the OpenAI page.
[00:36:45.520 --> 00:36:48.600]   I hit Apple A to select all.
[00:36:48.600 --> 00:36:54.240]   And then I said in chat.jpt, "Create a table with the pricing information rows.
[00:36:54.240 --> 00:36:55.420]   No summarization.
[00:36:55.420 --> 00:36:57.040]   No information not in this page.
[00:36:57.040 --> 00:36:59.440]   Every row should appear as a separate row in your output."
[00:36:59.440 --> 00:37:01.060]   And I hit paste.
[00:37:01.060 --> 00:37:02.600]   Now that was not very helpful to it.
[00:37:02.600 --> 00:37:09.960]   Because hitting paste, it's got the navbar, it's got lots of extra information at the
[00:37:09.960 --> 00:37:16.920]   bottom, it's got all of its footer, etc.
[00:37:16.920 --> 00:37:18.720]   But it's really good at this stuff.
[00:37:18.720 --> 00:37:20.920]   It did it first time.
[00:37:20.920 --> 00:37:22.440]   So there was the markdown table.
[00:37:22.440 --> 00:37:28.080]   So I copied and pasted that into Jupyter, and I got my markdown table.
[00:37:28.080 --> 00:37:34.320]   And so now you can see at a glance the cost of GPT-4, 3.5, etc.
[00:37:34.320 --> 00:37:38.600]   But then what I really wanted to do was show you that as a picture.
[00:37:38.600 --> 00:37:41.880]   So I just said, "Oh, chart the input row from this table."
[00:37:41.880 --> 00:37:44.200]   It just pasted the table back.
[00:37:44.200 --> 00:37:48.200]   And it did.
[00:37:48.200 --> 00:37:49.560]   So that's pretty amazing.
[00:37:49.560 --> 00:37:53.360]   Now, so let's talk about this pricing.
[00:37:53.360 --> 00:37:59.000]   So so far we've used chat-gpt, which costs 20 bucks a month, and there's no per token
[00:37:59.000 --> 00:38:00.400]   cost or anything.
[00:38:00.400 --> 00:38:04.560]   But if you want to use the API from Python or whatever, you have to pay per token.
[00:38:04.560 --> 00:38:13.480]   Which is approximately per word, maybe it's about one and a third tokens per word on average.
[00:38:13.480 --> 00:38:17.360]   Unfortunately in the chart, it did not include these headers, GPT-4, GPT-3.5.
[00:38:17.360 --> 00:38:21.940]   So these first two ones are GPT-4, and these two are GPT-3.5.
[00:38:21.940 --> 00:38:28.040]   So you can see the GPT-3.5 is way, way cheaper.
[00:38:28.040 --> 00:38:34.880]   And you can see it here, it's 0.03 versus 0.0015.
[00:38:34.880 --> 00:38:40.520]   So it's so cheap you can really play around with it and not worry.
[00:38:40.520 --> 00:38:43.280]   And I want to give you a sense of what that looks like.
[00:38:43.280 --> 00:38:51.260]   Okay, so why would you use the OpenAI API rather than chat-gpt?
[00:38:51.260 --> 00:38:53.080]   Because you can do it programmatically.
[00:38:53.080 --> 00:39:02.640]   So you can, you know, you can analyze data sets, you can do repetitive stuff.
[00:39:02.640 --> 00:39:05.620]   It's kind of like a different way of programming, you know.
[00:39:05.620 --> 00:39:09.520]   It's things that you can think of describing.
[00:39:09.520 --> 00:39:12.200]   But let's just look at the most simple example of what that looks like.
[00:39:12.200 --> 00:39:18.980]   So if you pip install OpenAI, then you can import chat-completion.
[00:39:18.980 --> 00:39:25.060]   And then you can say, okay, chat-completion.create using GPT-3.5 turbo.
[00:39:25.060 --> 00:39:27.920]   And then you can pass in a system message.
[00:39:27.920 --> 00:39:30.360]   This is basically the same as custom instructions.
[00:39:30.360 --> 00:39:36.360]   So okay, you're an Aussie LLM that uses Aussie slang and analogies wherever possible.
[00:39:36.360 --> 00:39:39.140]   And so you can see I'm passing in an array here of messages.
[00:39:39.140 --> 00:39:46.120]   So the first is the system message, and then the user message, which is "What is money?"
[00:39:46.120 --> 00:39:53.400]   So GPT-3.5 returns a big embedded dictionary.
[00:39:53.400 --> 00:39:59.000]   And the message content is, "Well, mate, money is like the oil that keeps the machinery of
[00:39:59.000 --> 00:40:00.520]   our economy running smoothly."
[00:40:00.520 --> 00:40:03.640]   There you go.
[00:40:03.640 --> 00:40:08.820]   Just like a koala loves its eucalyptus leaves, we humans can't survive without this stuff.
[00:40:08.820 --> 00:40:13.160]   So there's the Aussie LLM's view of what is money.
[00:40:13.160 --> 00:40:22.120]   So really, the main ones I pretty much always use are GPT-4 and GPT-3.5.
[00:40:22.120 --> 00:40:29.480]   GPT-4 is just so, so much better at anything remotely challenging.
[00:40:29.480 --> 00:40:31.120]   But obviously it's much more expensive.
[00:40:31.120 --> 00:40:35.800]   So rule of thumb, maybe try 3.5 turbo first.
[00:40:35.800 --> 00:40:36.880]   See how it goes.
[00:40:36.880 --> 00:40:38.720]   If you're happy with the results, then great.
[00:40:38.720 --> 00:40:42.200]   If you're not, pony up for the more expensive one.
[00:40:42.200 --> 00:40:49.320]   Okay, so I just created a little function here called response that will print out this
[00:40:49.320 --> 00:40:52.600]   nested thing.
[00:40:52.600 --> 00:40:57.720]   And so now, oh, and so then the other thing to point out here is that the result of this
[00:40:57.720 --> 00:41:04.400]   also has a usage field, which contains how many tokens was it?
[00:41:04.400 --> 00:41:06.600]   So it's about 150 tokens.
[00:41:06.600 --> 00:41:21.160]   So at .002 dollars per thousand tokens, for 150 tokens, means we just paid .03 cents,
[00:41:21.160 --> 00:41:26.480]   .0003 dollars to get that done.
[00:41:26.480 --> 00:41:29.340]   So as you can see, the cost is insignificant.
[00:41:29.340 --> 00:41:35.340]   If we were using GPT-4, it would be .03 per thousand.
[00:41:35.340 --> 00:41:39.760]   So it would be half a cent.
[00:41:39.760 --> 00:41:47.820]   So unless you're doing many thousands of GPT-4, you're not going to be even up into the dollars.
[00:41:47.820 --> 00:41:51.280]   And GPT-3.5 even more than that.
[00:41:51.280 --> 00:41:52.760]   But you know, keep an eye on it.
[00:41:52.760 --> 00:41:56.680]   OpenAI has a usage page and you can track your usage.
[00:41:56.680 --> 00:42:05.700]   Now what happens when we are, this is really important to understand, when we have a follow-up
[00:42:05.700 --> 00:42:07.760]   in the same conversation?
[00:42:07.760 --> 00:42:11.560]   How does that work?
[00:42:11.560 --> 00:42:14.460]   So we just asked what goat means.
[00:42:14.460 --> 00:42:21.900]   So for example, Michael Jordan is often referred to as the goat for his exceptional skills
[00:42:21.900 --> 00:42:25.400]   and accomplishments.
[00:42:25.400 --> 00:42:28.380]   And Elvis and the Beatles are referred to as goat due to their profound influence and
[00:42:28.380 --> 00:42:30.020]   achievement.
[00:42:30.020 --> 00:42:41.060]   So I could say, "What profound influence and achievements are you referring to?"
[00:42:41.060 --> 00:42:48.580]   Okay, well, I meant Elvis Presley and the Beatles did all these things.
[00:42:48.580 --> 00:42:49.780]   Now how does that work?
[00:42:49.780 --> 00:42:51.820]   How does this follow-up work?
[00:42:51.820 --> 00:42:57.580]   Well what happens is the entire conversation is passed back.
[00:42:57.580 --> 00:42:59.940]   And so we can actually do that here.
[00:42:59.940 --> 00:43:03.980]   So here is the same system prompt.
[00:43:03.980 --> 00:43:07.100]   Here is the same question.
[00:43:07.100 --> 00:43:09.980]   And then the answer comes back with role assistant.
[00:43:09.980 --> 00:43:12.220]   And I'm going to do something pretty cheeky.
[00:43:12.220 --> 00:43:17.220]   I'm going to pretend that it didn't say, "Money is like oil."
[00:43:17.220 --> 00:43:22.060]   I'm going to say, "Oh, you actually said money is like kangaroos."
[00:43:22.060 --> 00:43:24.620]   I thought, "What is it going to do?"
[00:43:24.620 --> 00:43:29.620]   So you can literally invent a conversation in which the language model said something
[00:43:29.620 --> 00:43:30.980]   different.
[00:43:30.980 --> 00:43:32.260]   Because this is actually how it's done.
[00:43:32.260 --> 00:43:36.580]   In a multi-stage conversation, there's no state.
[00:43:36.580 --> 00:43:38.700]   There's nothing stored on the server.
[00:43:38.700 --> 00:43:45.140]   You're passing back the entire conversation again and telling it what it told you.
[00:43:45.140 --> 00:43:48.700]   So I'm going to tell it, "It told me that money is like kangaroos."
[00:43:48.700 --> 00:43:50.900]   And then I'll ask the user, "Oh really?
[00:43:50.900 --> 00:43:51.900]   In what way?"
[00:43:51.900 --> 00:43:58.540]   And it's kind of cool because you can see how it convinces you of something I just invented.
[00:43:58.540 --> 00:44:00.780]   "Oh, let me break it down for you, Cubber.
[00:44:00.780 --> 00:44:04.580]   Just like kangaroos hop around and carry their joeys in their pouch, money is a means of
[00:44:04.580 --> 00:44:06.020]   carrying value around."
[00:44:06.020 --> 00:44:08.260]   So there you go.
[00:44:08.260 --> 00:44:11.300]   Make your own analogy.
[00:44:11.300 --> 00:44:17.540]   So I'll create a little function here that just puts these things together for us.
[00:44:17.540 --> 00:44:22.660]   System message if there is one, the user message, and returns the completion.
[00:44:22.660 --> 00:44:26.060]   And so now we can ask it, "What's the meaning of life?"
[00:44:26.060 --> 00:44:28.220]   Passing in the Aussie system prompt.
[00:44:28.220 --> 00:44:32.900]   "The meaning of life is like trying to catch a wave on a sunny day at Bondi Beach."
[00:44:32.900 --> 00:44:34.540]   Okay, there you go.
[00:44:34.540 --> 00:44:37.020]   So what do you need to be aware of?
[00:44:37.020 --> 00:44:40.480]   Well, as I said, one thing is keep an eye on your usage.
[00:44:40.480 --> 00:44:45.060]   If you're doing it, you know, hundreds or thousands of times in a loop, keep an eye
[00:44:45.060 --> 00:44:48.060]   on not spending too much money.
[00:44:48.060 --> 00:44:52.900]   But also if you're doing it too fast, particularly the first day or two, you've got an account,
[00:44:52.900 --> 00:44:57.540]   you're likely to hit the limits for the API.
[00:44:57.540 --> 00:45:03.060]   And so the limits initially are pretty low.
[00:45:03.060 --> 00:45:10.100]   As you can see, three requests per minute.
[00:45:10.100 --> 00:45:13.660]   That's for free users, page users, first 48 hours.
[00:45:13.660 --> 00:45:16.980]   And after that, it starts going up and you can always ask for more.
[00:45:16.980 --> 00:45:21.820]   I just mention this because you're going to want to have a function that keeps an eye
[00:45:21.820 --> 00:45:23.340]   on that.
[00:45:23.340 --> 00:45:28.880]   And so what I did is I actually just went to Bing, which has a somewhat crappy version
[00:45:28.880 --> 00:45:32.800]   of GPT-4 nowadays, but it can still do basic stuff for free.
[00:45:32.800 --> 00:45:41.680]   And I said, please show me Python code to call the OpenAI API and handle rate limits.
[00:45:41.680 --> 00:45:42.680]   And it wrote this code.
[00:45:42.680 --> 00:45:52.680]   It's got a try, checks your rate limit errors, grabs the retry after, sleeps for that long,
[00:45:52.680 --> 00:45:55.160]   and calls itself.
[00:45:55.160 --> 00:46:01.720]   And so now we can use that to ask, for example, what's the world's funniest joke?
[00:46:01.720 --> 00:46:03.960]   There we go.
[00:46:03.960 --> 00:46:08.120]   There's the world's funniest joke.
[00:46:08.120 --> 00:46:18.600]   So that's like the basic stuff you need to get started using the OpenAI LLMs.
[00:46:18.600 --> 00:46:25.440]   And yeah, it's definitely suggest spending plenty of time with that so that you feel
[00:46:25.440 --> 00:46:33.840]   like you're really a LLM using expert.
[00:46:33.840 --> 00:46:35.680]   So what else can we do?
[00:46:35.680 --> 00:46:41.280]   Well let's create our own code interpreter that runs inside Jupyter.
[00:46:41.280 --> 00:46:49.120]   And so to do this, we're going to take advantage of a really nifty thing called function calling,
[00:46:49.120 --> 00:46:52.040]   which is provided by the OpenAI API.
[00:46:52.040 --> 00:46:58.360]   And in function calling, when we call our askGPT function, which is this little one
[00:46:58.360 --> 00:47:03.520]   here, we have room to pass in some keyword arguments that will be just passed along to
[00:47:03.520 --> 00:47:06.040]   chat completion.create.
[00:47:06.040 --> 00:47:12.680]   And one of those keyword arguments you can pass is functions.
[00:47:12.680 --> 00:47:14.720]   What on earth is that?
[00:47:14.720 --> 00:47:23.280]   Functions tells OpenAI about tools that you have, about functions that you have.
[00:47:23.280 --> 00:47:33.720]   So for example, I created a really simple function called sums, and it adds two things.
[00:47:33.720 --> 00:47:37.920]   In fact it adds two ints.
[00:47:37.920 --> 00:47:46.160]   I'm going to pass that function to chat completion.create.
[00:47:46.160 --> 00:47:49.420]   Now you can't pass a Python function directly.
[00:47:49.420 --> 00:47:54.080]   You actually have to pass what's called the JSON schema.
[00:47:54.080 --> 00:47:57.360]   So you have to pass the schema for the function.
[00:47:57.360 --> 00:48:06.000]   So I created this nifty little function that you're welcome to borrow, which uses pydantic
[00:48:06.000 --> 00:48:14.920]   and also Python's inspect module to automatically take a Python function and return the schema
[00:48:14.920 --> 00:48:15.920]   for it.
[00:48:15.920 --> 00:48:17.920]   And so this is actually what's going to get passed to OpenAI.
[00:48:17.920 --> 00:48:22.220]   So it's going to know that there's a function called sums, it's going to know what it does,
[00:48:22.220 --> 00:48:29.840]   and it's going to know what parameters it takes, what the defaults are, and what's required.
[00:48:29.840 --> 00:48:34.560]   So this is like, when I first heard about this, I found this a bit mind-bending, because
[00:48:34.560 --> 00:48:38.200]   this is so different to how we normally program computers.
[00:48:38.200 --> 00:48:42.560]   The key thing for programming the computer here actually is the docstring.
[00:48:42.560 --> 00:48:48.080]   This is the thing that gpt4 will look at and say, "Oh, what does this function do?"
[00:48:48.080 --> 00:48:52.360]   So it's critical that this describes exactly what the function does.
[00:48:52.360 --> 00:48:58.160]   And so if I then say, "What is 6+3?"
[00:48:58.160 --> 00:49:05.360]   I really wanted to make sure it actually did it here, so I gave it lots of prompts to say
[00:49:05.360 --> 00:49:10.160]   -- because obviously it knows how to do it itself without calling sums.
[00:49:10.160 --> 00:49:15.640]   So it'll only use your functions if it feels it needs to, which is a weird concept.
[00:49:15.640 --> 00:49:20.440]   I mean, I guess "feels" is not a great word to use, but you kind of have to anthropomorphize
[00:49:20.440 --> 00:49:25.480]   these things a little bit, because they don't behave like normal computer programs.
[00:49:25.480 --> 00:49:33.080]   So if I ask gpt, "What is 6+3?" and tell it that there's a function called sums, then
[00:49:33.080 --> 00:49:37.640]   it does not actually return the number 9.
[00:49:37.640 --> 00:49:41.280]   Instead it returns something saying, "Please call a function.
[00:49:41.280 --> 00:49:45.760]   Call this function and pass it these arguments."
[00:49:45.760 --> 00:49:49.680]   So if I print it out, there's the arguments.
[00:49:49.680 --> 00:49:57.420]   So I created a little function called call_function, and it goes into the result of OpenAI, grabs
[00:49:57.420 --> 00:50:04.160]   the function call, checks that the name is something that it's allowed to do, grabs it
[00:50:04.160 --> 00:50:10.860]   from the global system table, and calls it, passing in the parameters.
[00:50:10.860 --> 00:50:22.640]   And so if I now say, "Okay, call the function that we got back," we finally get 9.
[00:50:22.640 --> 00:50:25.560]   So this is a very simple example.
[00:50:25.560 --> 00:50:29.360]   It's not really doing anything that useful, but what we could do now is we can create
[00:50:29.360 --> 00:50:35.640]   a much more powerful function called Python.
[00:50:35.640 --> 00:50:46.720]   And the Python function executes code using Python, and returns the result.
[00:50:46.720 --> 00:50:53.640]   Now of course, I didn't want my computer to run arbitrary Python code that gpt4 told it
[00:50:53.640 --> 00:50:57.060]   to without checking, so I just got it to check first.
[00:50:57.060 --> 00:51:01.660]   So say, "Oh, you sure you want to do this?"
[00:51:01.660 --> 00:51:09.740]   So now I can say, "Ask gpt, what is 12!
[00:51:09.740 --> 00:51:10.740]   System prompt."
[00:51:10.740 --> 00:51:14.380]   You can use Python for any required computations, and say, "Okay, here's a function you've got
[00:51:14.380 --> 00:51:15.660]   available.
[00:51:15.660 --> 00:51:18.200]   It's the Python function."
[00:51:18.200 --> 00:51:26.380]   So if I now call this, it will pass me back again a completion object, and here it's going
[00:51:26.380 --> 00:51:33.580]   to say, "Okay, I want you to call Python passing in this argument."
[00:51:33.580 --> 00:51:39.100]   And when I do, it's going to go, "Import math, result equals blah," and then return result.
[00:51:39.100 --> 00:51:40.100]   Do I want to do that?
[00:51:40.100 --> 00:51:41.100]   Yes, I do.
[00:51:41.100 --> 00:51:45.820]   And there it is.
[00:51:45.820 --> 00:51:48.860]   Now there's one more step which we can optionally do.
[00:51:48.860 --> 00:51:52.740]   I mean, we've got the answer we wanted, but often we want the answer in more of a chat
[00:51:52.740 --> 00:52:00.980]   format, and so the way to do that is to, again, repeat everything that you've passed into
[00:52:00.980 --> 00:52:08.660]   so far, but then instead of adding an assistant role response, we have to provide a function
[00:52:08.660 --> 00:52:16.580]   role response, and simply put in here the result we got back from the function.
[00:52:16.580 --> 00:52:28.740]   And if we do that, we now get the prose response, "12 factorial is equal to 470 and a million,
[00:52:28.740 --> 00:52:30.180]   1,600."
[00:52:30.180 --> 00:52:42.100]   Now functions like Python, you can still ask it about non-Python things, and it just ignores
[00:52:42.100 --> 00:52:43.880]   it if you don't need it, right?
[00:52:43.880 --> 00:52:48.960]   So you can have a whole bunch of functions available that you've built to do whatever
[00:52:48.960 --> 00:52:57.340]   you need for the stuff which the language model isn't familiar with.
[00:52:57.340 --> 00:53:03.980]   And it'll still solve whatever it can on its own, and use your tools, use your functions
[00:53:03.980 --> 00:53:07.700]   where possible.
[00:53:07.700 --> 00:53:14.980]   Okay, so we have built our own code interpreter from scratch.
[00:53:14.980 --> 00:53:22.440]   I think that's pretty amazing.
[00:53:22.440 --> 00:53:32.580]   So that is what you can do with -- or some of the stuff you can do with OpenAI.
[00:53:32.580 --> 00:53:36.580]   What about stuff that you can do on your own computer?
[00:53:36.580 --> 00:53:45.820]   Well, to use a language model on your own computer, you're going to need to use a GPU.
[00:53:45.820 --> 00:53:50.600]   So I guess the first thing to think about is, like, do you want this?
[00:53:50.600 --> 00:53:55.460]   Does it make sense to do stuff on your own computer?
[00:53:55.460 --> 00:53:58.860]   What are the benefits?
[00:53:58.860 --> 00:54:06.860]   There are not any open source models that are as good yet as GPT-4.
[00:54:06.860 --> 00:54:11.460]   And I would have to say also, like, actually OpenAI's pricing's really pretty good.
[00:54:11.460 --> 00:54:17.500]   So it's not immediately obvious that you definitely want to kind of go in-house.
[00:54:17.500 --> 00:54:20.080]   But there's lots of reasons you might want to.
[00:54:20.080 --> 00:54:25.100]   And we'll look at some examples of them today.
[00:54:25.100 --> 00:54:30.860]   One example you might want to go in-house is that you want to be able to ask questions
[00:54:30.860 --> 00:54:37.700]   about your proprietary documents or about information after September 2021, the knowledge
[00:54:37.700 --> 00:54:39.260]   cutoff.
[00:54:39.260 --> 00:54:43.240]   Or you might want to create your own model that's particularly good at solving the kinds
[00:54:43.240 --> 00:54:46.680]   of problems that you need to solve using fine tuning.
[00:54:46.680 --> 00:54:51.940]   And these are all things that you absolutely can get better than GPT-4 performance at work
[00:54:51.940 --> 00:54:57.300]   or at home without too much money or trouble.
[00:54:57.300 --> 00:55:00.820]   So these are the situations in which you might want to go down this path.
[00:55:00.820 --> 00:55:04.620]   And so you don't necessarily have to buy a GPU.
[00:55:04.620 --> 00:55:12.020]   On Kaggle, they will give you a notebook with two quite old GPUs attached and very little
[00:55:12.020 --> 00:55:13.180]   RAM.
[00:55:13.180 --> 00:55:14.820]   But it's something.
[00:55:14.820 --> 00:55:16.420]   Or you can use Colab.
[00:55:16.420 --> 00:55:23.300]   And on Colab, you can get much better GPUs than Kaggle has and more RAM, particularly
[00:55:23.300 --> 00:55:28.660]   if you pay a monthly subscription fee.
[00:55:28.660 --> 00:55:33.860]   So those are some options for free or low cost.
[00:55:33.860 --> 00:55:44.500]   You can also, of course, go to one of the many GPU server providers.
[00:55:44.500 --> 00:55:49.940]   And they change all the time as to what's good or what's not.
[00:55:49.940 --> 00:55:53.100]   RunPod is one example.
[00:55:53.100 --> 00:55:59.660]   And you can see, you know, if you want the biggest and best machine, you're talking $34
[00:55:59.660 --> 00:56:00.660]   an hour.
[00:56:00.660 --> 00:56:02.020]   So it gets pretty expensive.
[00:56:02.020 --> 00:56:04.940]   But you can certainly get things a lot cheaper.
[00:56:04.940 --> 00:56:08.020]   80 cents an hour.
[00:56:08.020 --> 00:56:12.020]   Lambda Labs is often pretty good.
[00:56:12.020 --> 00:56:22.340]   You know, it's really hard at the moment to actually find.
[00:56:22.340 --> 00:56:23.340]   Let's see.
[00:56:23.340 --> 00:56:24.340]   Pricing.
[00:56:24.340 --> 00:56:25.580]   To actually find people that have them available.
[00:56:25.580 --> 00:56:27.140]   So they've got lots listed here.
[00:56:27.140 --> 00:56:32.700]   But they often have none or very few available.
[00:56:32.700 --> 00:56:37.060]   There's also something pretty interesting called Fast AI.
[00:56:37.060 --> 00:56:46.940]   Which basically lets you use other people's computers when they're not using them.
[00:56:46.940 --> 00:56:55.740]   And as you can see, you know, they tend to be much cheaper than other folks.
[00:56:55.740 --> 00:56:58.100]   And they tend to have better availability as well.
[00:56:58.100 --> 00:57:02.100]   But of course, for sensitive stuff, you don't want to be running it on some rando's computer.
[00:57:02.100 --> 00:57:06.060]   So anyway, there's a few options for renting stuff.
[00:57:06.060 --> 00:57:08.620]   I think if you can, it's worth buying something.
[00:57:08.620 --> 00:57:14.220]   And definitely the one to buy at the moment is the GTX 3090 used.
[00:57:14.220 --> 00:57:19.540]   You can generally get them from eBay for like 700 bucks or so.
[00:57:19.540 --> 00:57:23.780]   A 4090 isn't really better for language models.
[00:57:23.780 --> 00:57:25.700]   Even though it's a newer GPU.
[00:57:25.700 --> 00:57:30.420]   The reason for that is that language models are all about memory speed.
[00:57:30.420 --> 00:57:32.900]   How quickly can you get in and stuff in and out of memory.
[00:57:32.900 --> 00:57:34.740]   Rather than how fast is the processor.
[00:57:34.740 --> 00:57:37.940]   And that hasn't really improved a whole lot.
[00:57:37.940 --> 00:57:40.540]   So the 2000 bucks.
[00:57:40.540 --> 00:57:43.220]   The other thing as well as memory speed is memory size.
[00:57:43.220 --> 00:57:44.900]   24 gigs.
[00:57:44.900 --> 00:57:47.060]   It doesn't quite cut it for a lot of things.
[00:57:47.060 --> 00:57:49.860]   So you'd probably want to get two of these GPUs.
[00:57:49.860 --> 00:57:54.620]   So you're talking like $1500 or so.
[00:57:54.620 --> 00:57:57.900]   Or you can get a 48 gig RAM GPU.
[00:57:57.900 --> 00:58:00.300]   It's called an A6000.
[00:58:00.300 --> 00:58:03.900]   But this is going to cost you more like five grand.
[00:58:03.900 --> 00:58:08.940]   So again, getting two of these is going to be a better deal.
[00:58:08.940 --> 00:58:14.180]   This is not going to be faster than these either.
[00:58:14.180 --> 00:58:17.620]   Or funnily enough, you could just get a Mac with a lot of RAM.
[00:58:17.620 --> 00:58:20.980]   Particularly if you get an M2 Ultra.
[00:58:20.980 --> 00:58:26.460]   Macs have, particularly the M2 Ultra, has pretty fast memory.
[00:58:26.460 --> 00:58:30.140]   It's still going to be way slower than using an Nvidia card.
[00:58:30.140 --> 00:58:34.620]   That's going to be like, you're going to be able to get, you know, like I think 192 gig
[00:58:34.620 --> 00:58:37.460]   or something.
[00:58:37.460 --> 00:58:42.060]   So it's not a terrible option.
[00:58:42.060 --> 00:58:43.340]   Particularly if you're not training models.
[00:58:43.340 --> 00:58:51.940]   You're just wanting to use other existing trained models.
[00:58:51.940 --> 00:58:58.660]   So anyway, most people who do this stuff seriously, almost everybody, has Nvidia cards.
[00:58:58.660 --> 00:59:05.140]   So then what we're going to be using is a library called Transformers from Hugging Face.
[00:59:05.140 --> 00:59:10.820]   And the reason for that is that basically people upload lots of pre-trained models or
[00:59:10.820 --> 00:59:14.380]   fine-tuned models up to the Hugging Face hub.
[00:59:14.380 --> 00:59:20.940]   And in fact, there's even a leaderboard where you can see which are the best models.
[00:59:20.940 --> 00:59:27.980]   Now this is a really fraught area.
[00:59:27.980 --> 00:59:30.220]   So at the moment this one is meant to be the best model.
[00:59:30.220 --> 00:59:33.220]   It has the highest average score.
[00:59:33.220 --> 00:59:34.220]   And maybe it is good.
[00:59:34.220 --> 00:59:37.660]   I haven't actually used this particular model.
[00:59:37.660 --> 00:59:38.660]   Or maybe it's not.
[00:59:38.660 --> 00:59:40.140]   I actually have no idea.
[00:59:40.140 --> 00:59:47.180]   Because the problem is, these metrics are not particularly well aligned with real life
[00:59:47.180 --> 00:59:51.100]   usage for all kinds of reasons.
[00:59:51.100 --> 00:59:55.420]   And also sometimes you get something called leakage, which means that sometimes some of
[00:59:55.420 --> 01:00:01.420]   the questions from these things actually leaks through to some of the training sets.
[01:00:01.420 --> 01:00:05.540]   So you can get as a rule of thumb what to use from here.
[01:00:05.540 --> 01:00:09.220]   But you should always try things.
[01:00:09.220 --> 01:00:13.460]   And you can also say, you know, these ones are all -- this 70B here, that tells you how
[01:00:13.460 --> 01:00:14.460]   big it is.
[01:00:14.460 --> 01:00:19.100]   This is a 70 billion parameter model.
[01:00:19.100 --> 01:00:25.620]   So generally speaking, for the kinds of GPUs we're talking about, you'll be wanting no
[01:00:25.620 --> 01:00:30.600]   bigger than 13B, and quite often 7B.
[01:00:30.600 --> 01:00:36.620]   So let's see if we can find -- here's a 13B model, for example.
[01:00:36.620 --> 01:00:44.860]   All right, so you can find models to try out from things like this leaderboard.
[01:00:44.860 --> 01:00:49.060]   And there's also a really great leaderboard called FastEval, which I like a lot.
[01:00:49.060 --> 01:00:56.180]   Because it focuses on some more sophisticated evaluation methods, such as this chain of
[01:00:56.180 --> 01:00:58.620]   thought evaluation method.
[01:00:58.620 --> 01:01:01.980]   So I kind of trust these a little bit more.
[01:01:01.980 --> 01:01:07.220]   And these are also -- GSM 8K is a difficult math benchmark.
[01:01:07.220 --> 01:01:10.780]   Big Bench Hard, and so forth.
[01:01:10.780 --> 01:01:17.460]   So yeah, so, you know, Stable Beluga2, WizardMath 13B, DolphinLlama 13B, etc.
[01:01:17.460 --> 01:01:19.860]   These would all be good options.
[01:01:19.860 --> 01:01:25.340]   Yeah, so you need to pick a model.
[01:01:25.340 --> 01:01:31.940]   And at the moment, nearly all the good models are based on MetasLlama2.
[01:01:31.940 --> 01:01:34.500]   So when I say "based on", what does that mean?
[01:01:34.500 --> 01:01:40.460]   Well what that means is this model here, Llama2 7B.
[01:01:40.460 --> 01:01:42.660]   So it's a llama model.
[01:01:42.660 --> 01:01:44.220]   That's just the name Meta called it.
[01:01:44.220 --> 01:01:46.300]   This is their version 2 of Llama.
[01:01:46.300 --> 01:01:48.180]   This is their 7 billion size one.
[01:01:48.180 --> 01:01:50.460]   It's the smallest one that they make.
[01:01:50.460 --> 01:01:54.060]   And specifically, these weights have been created for HuggingFace, so you can load it
[01:01:54.060 --> 01:01:56.340]   with the HuggingFace transformers.
[01:01:56.340 --> 01:02:00.260]   And this model has only got as far as here.
[01:02:00.260 --> 01:02:02.340]   It's done the language model for pre-training.
[01:02:02.340 --> 01:02:09.100]   It's done none of the instruction tuning, and none of the RLHF.
[01:02:09.100 --> 01:02:15.060]   So we would need to fine-tune it to really get it to do much useful.
[01:02:15.060 --> 01:02:22.660]   So we can just say, okay, automatically create the appropriate model for language model.
[01:02:22.660 --> 01:02:28.260]   So causalLM basically refers to that ULM fit stage 1 process.
[01:02:28.260 --> 01:02:29.700]   Or stage 2, in fact.
[01:02:29.700 --> 01:02:36.540]   So get the pre-trained model from this name, meta-llama-llama2-blah-blah.
[01:02:36.540 --> 01:02:47.940]   Now generally speaking, we use 16-bit floating point numbers nowadays.
[01:02:47.940 --> 01:02:51.940]   But if you think about it, 16-bit is 2 bytes.
[01:02:51.940 --> 01:03:01.100]   So 7b times 2, it's going to be 14 gigabytes just to load in the weights.
[01:03:01.100 --> 01:03:05.940]   So you've got to have a decent model to be able to do that.
[01:03:05.940 --> 01:03:10.820]   Perhaps surprisingly, you can actually just cast it to 8-bit, and it still works pretty
[01:03:10.820 --> 01:03:14.180]   well thanks to something called discretization.
[01:03:14.180 --> 01:03:16.940]   So let's try that.
[01:03:16.940 --> 01:03:18.620]   So remember, this is just a language model.
[01:03:18.620 --> 01:03:20.100]   It can only complete sentences.
[01:03:20.100 --> 01:03:22.840]   You can't ask it a question and expect a great answer.
[01:03:22.840 --> 01:03:24.340]   So let's just give it the start of a sentence.
[01:03:24.340 --> 01:03:26.780]   Jeremy Howard is a...
[01:03:26.780 --> 01:03:28.620]   And so we need the right tokenizer.
[01:03:28.620 --> 01:03:32.260]   So this will automatically create the right kind of tokenizer for this model.
[01:03:32.260 --> 01:03:36.540]   We can grab the tokens as PyTorch.
[01:03:36.540 --> 01:03:41.500]   Here they are.
[01:03:41.500 --> 01:03:46.640]   And just to confirm, if we decode them back again, we get back the original plus a special
[01:03:46.640 --> 01:03:50.340]   token to say this is the start of a document.
[01:03:50.340 --> 01:03:53.140]   And so we can now call generate.
[01:03:53.140 --> 01:04:02.180]   So generate will auto-regressively, so call the model again and again, passing its previous
[01:04:02.180 --> 01:04:08.540]   result back as the next input.
[01:04:08.540 --> 01:04:10.920]   And I'm just going to do that 15 times.
[01:04:10.920 --> 01:04:13.500]   So you can write this for loop yourself.
[01:04:13.500 --> 01:04:14.660]   This isn't doing anything fancy.
[01:04:14.660 --> 01:04:19.420]   In fact, I would recommend writing this yourself to make sure that you know how, that it all
[01:04:19.420 --> 01:04:24.140]   works okay.
[01:04:24.140 --> 01:04:26.560]   We have to put those tokens on the GPU.
[01:04:26.560 --> 01:04:29.780]   And at the end, I recommend putting them back onto the CPU, the result.
[01:04:29.780 --> 01:04:31.720]   And here are the tokens.
[01:04:31.720 --> 01:04:32.720]   Not very interesting.
[01:04:32.720 --> 01:04:35.640]   So we have to decode them using the tokenizer.
[01:04:35.640 --> 01:04:41.540]   And so the first 25, sorry, first 15 tokens are Jeremy Howard is a 28 year old Australian
[01:04:41.540 --> 01:04:43.060]   AI researcher and entrepreneur.
[01:04:43.060 --> 01:04:47.420]   Okay, well, 28 years old is not exactly correct, but we'll call it close enough.
[01:04:47.420 --> 01:04:48.500]   I like that.
[01:04:48.500 --> 01:04:51.920]   Thank you very much, Lama7b.
[01:04:51.920 --> 01:04:57.040]   So okay, so we've got a language model completing sentences.
[01:04:57.040 --> 01:05:02.340]   It took one and a third seconds.
[01:05:02.340 --> 01:05:06.280]   And that's a bit slower than it could be because we used 8-bit.
[01:05:06.280 --> 01:05:12.660]   If we use 16-bit, there's a special thing called bfloat16, which is a really great 16-bit
[01:05:12.660 --> 01:05:18.860]   floating point format that's usable on any somewhat recent NVIDIA GPU.
[01:05:18.860 --> 01:05:23.940]   Now if we use it, it's going to take twice as much RAM as we discussed.
[01:05:23.940 --> 01:05:26.100]   But look at the time.
[01:05:26.100 --> 01:05:30.820]   It's come down to 390 milliseconds.
[01:05:30.820 --> 01:05:34.880]   Now there is a better option still than even that.
[01:05:34.880 --> 01:05:43.180]   There's a different kind of discretization called GPTQ, where a model is carefully optimized
[01:05:43.180 --> 01:05:51.940]   to work with four or eight or other, you know, lower precision data automatically.
[01:05:51.940 --> 01:06:01.100]   And this particular person known as the bloke is fantastic at taking popular models, running
[01:06:01.100 --> 01:06:07.220]   that optimization process, and then uploading the results back to Hackingface.
[01:06:07.220 --> 01:06:12.060]   So we can use this GPTQ version.
[01:06:12.060 --> 01:06:15.620]   And internally, this is actually going to use -- I'm not sure exactly how many bits
[01:06:15.620 --> 01:06:16.620]   this particular one is.
[01:06:16.620 --> 01:06:19.140]   I think it's probably going to be four bits.
[01:06:19.140 --> 01:06:22.260]   But it's going to be much more optimized.
[01:06:22.260 --> 01:06:23.260]   And so look at this.
[01:06:23.260 --> 01:06:24.260]   270 milliseconds.
[01:06:24.260 --> 01:06:29.900]   It's actually faster than 16-bit.
[01:06:29.900 --> 01:06:34.780]   Even though internally it's actually casting it up to 16-bit each layer to do it.
[01:06:34.780 --> 01:06:38.620]   That's because there's a lot less memory moving around.
[01:06:38.620 --> 01:06:43.420]   And to confirm, in fact what we could even do now is we could go up to 13-bit.
[01:06:43.420 --> 01:06:44.420]   Easy.
[01:06:44.420 --> 01:06:49.460]   And in fact it's still faster than the 7-bit, now that we're using the GPTQ version.
[01:06:49.460 --> 01:06:52.620]   So this is a really helpful tip.
[01:06:52.620 --> 01:06:54.460]   So let's put all those things together.
[01:06:54.460 --> 01:06:56.580]   The tokenizer, the generate, the batch decode.
[01:06:56.580 --> 01:06:58.780]   We'll call this gen for generate.
[01:06:58.780 --> 01:07:03.420]   And so we can now use the 13-bit GPTQ model.
[01:07:03.420 --> 01:07:04.420]   And let's try this.
[01:07:04.420 --> 01:07:08.100]   Jeremy Howard is a -- so it's got to 50 tokens so fast.
[01:07:08.100 --> 01:07:10.100]   16-year veteran of Silicon Valley.
[01:07:10.100 --> 01:07:13.420]   Co-founder of Kaggle, a marketplace for predictive model.
[01:07:13.420 --> 01:07:16.140]   His company, Kaggle.com, has become a data science competition.
[01:07:16.140 --> 01:07:17.780]   I don't know what I was going to say.
[01:07:17.780 --> 01:07:19.660]   But anyway, it's on the right track.
[01:07:19.660 --> 01:07:21.980]   I was actually there for 10 years, not 16.
[01:07:21.980 --> 01:07:25.820]   But that's all right.
[01:07:25.820 --> 01:07:26.820]   Okay.
[01:07:26.820 --> 01:07:30.060]   So it's looking good.
[01:07:30.060 --> 01:07:35.100]   But probably a lot of the time we're going to be interested in, you know, asking questions
[01:07:35.100 --> 01:07:36.880]   or using instructions.
[01:07:36.880 --> 01:07:41.980]   So Stability AI has this nice series called Stable Beluga, including a small 7B one and
[01:07:41.980 --> 01:07:44.740]   other bigger ones.
[01:07:44.740 --> 01:07:46.180]   And these are all based on Lama 2.
[01:07:46.180 --> 01:07:48.220]   But these have been instruction tuned.
[01:07:48.220 --> 01:07:50.340]   They might even have been RLHDFed.
[01:07:50.340 --> 01:07:53.040]   I can't remember now.
[01:07:53.040 --> 01:07:57.260]   So we can create a Stable Beluga model.
[01:07:57.260 --> 01:08:03.420]   And now something really important that I keep forgetting, everybody keeps forgetting,
[01:08:03.420 --> 01:08:16.340]   is during the instruction tuning process, during the instruction tuning process, the
[01:08:16.340 --> 01:08:26.120]   instructions that are passed in actually are, they don't just appear like this.
[01:08:26.120 --> 01:08:29.160]   They actually always are in a particular format.
[01:08:29.160 --> 01:08:35.220]   And the format, believe it or not, changes quite a bit from fine tune to fine tune.
[01:08:35.220 --> 01:08:45.200]   And so you have to go to the web page for the model and scroll down to find out what
[01:08:45.200 --> 01:08:47.400]   the prompt format is.
[01:08:47.400 --> 01:08:48.560]   So here's the prompt format.
[01:08:48.560 --> 01:09:00.400]   So I generally just copy it and then I paste it into Python, which I did here, and created
[01:09:00.400 --> 01:09:08.720]   a function called make_prompt that used the exact same format that it said to use.
[01:09:08.720 --> 01:09:11.600]   And so now if I want to say, "Who is Jeremy Howard?"
[01:09:11.600 --> 01:09:18.200]   I can call gen again, that was that function I created up here, and make the correct prompt
[01:09:18.200 --> 01:09:20.680]   from that question.
[01:09:20.680 --> 01:09:21.680]   And then it returns back.
[01:09:21.680 --> 01:09:25.080]   Okay, so you can see here all this prefix.
[01:09:25.080 --> 01:09:27.520]   This is the system instruction.
[01:09:27.520 --> 01:09:28.520]   This is my question.
[01:09:28.520 --> 01:09:33.480]   And then the assistant says, "Jeremy Howard's an Australian entrepreneur, computer scientist,
[01:09:33.480 --> 01:09:36.480]   co-founder of machine learning and deep learning company Fast.ai."
[01:09:36.480 --> 01:09:38.960]   Okay, so this one's actually all correct.
[01:09:38.960 --> 01:09:46.360]   So it's getting better by using an actual instruction tune model.
[01:09:46.360 --> 01:09:48.440]   And so we could then start to scale up.
[01:09:48.440 --> 01:09:50.480]   So we could use the 13b.
[01:09:50.480 --> 01:09:55.600]   And in fact, we looked briefly at this OpenOrca dataset earlier.
[01:09:55.600 --> 01:10:01.760]   So LLAMA2 has been fine-tuned on OpenOrca, and then also fine-tuned on another really
[01:10:01.760 --> 01:10:04.680]   great dataset called Platypus.
[01:10:04.680 --> 01:10:10.120]   And so the whole thing together is the OpenOrca Platypus, and then this is going to be the
[01:10:10.120 --> 01:10:11.120]   bigger 13b.
[01:10:11.120 --> 01:10:15.520]   GPTQ means it's going to be quantized.
[01:10:15.520 --> 01:10:17.800]   So that's got a different format.
[01:10:17.800 --> 01:10:20.240]   Okay, a different prompt format.
[01:10:20.240 --> 01:10:24.680]   So again, we can scroll down and see what the prompt format is.
[01:10:24.680 --> 01:10:27.480]   There it is.
[01:10:27.480 --> 01:10:37.600]   And so we can create a function called makeOpenOrcaPrompt that has that prompt format.
[01:10:37.600 --> 01:10:40.040]   And so now we can say, okay, who is Jeremy Howard?
[01:10:40.040 --> 01:10:42.200]   And now I've become British, which is kind of true.
[01:10:42.200 --> 01:10:45.760]   I was born in England, but I moved to Australia.
[01:10:45.760 --> 01:10:46.760]   Professional poker player.
[01:10:46.760 --> 01:10:48.280]   No, definitely not that.
[01:10:48.280 --> 01:10:53.240]   Co-founding several companies, including Fast.ai.
[01:10:53.240 --> 01:10:54.240]   Also Kaggle.
[01:10:54.240 --> 01:10:55.240]   Okay, so not bad.
[01:10:55.360 --> 01:10:58.040]   It was acquired by Google in 2017.
[01:10:58.040 --> 01:10:59.440]   Probably something around there.
[01:10:59.440 --> 01:11:09.880]   Okay, so you can see we've got our own models giving us some pretty good information.
[01:11:09.880 --> 01:11:11.080]   How do we make it even better?
[01:11:11.080 --> 01:11:19.120]   You know, because it's still hallucinating, you know.
[01:11:19.120 --> 01:11:25.360]   And you know, LLAMA2, I think, has been trained with more up-to-date information than GPT-4.
[01:11:25.360 --> 01:11:30.360]   It doesn't have the September 2021 cutoff.
[01:11:30.360 --> 01:11:33.400]   But it, you know, it's still got a knowledge cutoff.
[01:11:33.400 --> 01:11:36.040]   You know, we would like to be able to use the most up-to-date information.
[01:11:36.040 --> 01:11:41.400]   We want to use the right information to answer these questions as well as possible.
[01:11:41.400 --> 01:11:46.480]   So to do this, we can use something called retrieval augmented generation.
[01:11:46.480 --> 01:11:55.120]   So what happens with retrieval augmented generation is when we take the question we've been asked,
[01:11:55.120 --> 01:12:04.600]   like "who is Jeremy Howd?" and then we say, "okay, let's try and search for documents
[01:12:04.600 --> 01:12:08.160]   that may help us answer that question."
[01:12:08.160 --> 01:12:12.200]   So obviously we would expect, for example, Wikipedia to be useful.
[01:12:12.200 --> 01:12:20.680]   And then what we do is we say, "okay, with that information, let's now see if we can
[01:12:20.680 --> 01:12:29.120]   tell the language model about what we found and then have it answer the question."
[01:12:29.120 --> 01:12:30.400]   So let me show you.
[01:12:30.400 --> 01:12:36.880]   So let's actually grab a Wikipedia Python package.
[01:12:36.880 --> 01:12:42.840]   We will scrape Wikipedia, grabbing the Jeremy Howd webpage.
[01:12:42.840 --> 01:12:49.000]   And so here's the start of the Jeremy Howd Wikipedia page.
[01:12:49.000 --> 01:12:51.040]   It has 613 words.
[01:12:51.040 --> 01:12:55.120]   Now generally speaking, these open source models will have a context length of about
[01:12:55.120 --> 01:12:56.860]   2000 or 4000.
[01:12:56.860 --> 01:13:00.120]   So the context length is how many tokens can it handle.
[01:13:00.120 --> 01:13:01.120]   So that's fine.
[01:13:01.120 --> 01:13:03.640]   It'll be able to handle this webpage.
[01:13:03.640 --> 01:13:06.760]   And what we're going to do is we're going to ask it the question.
[01:13:06.760 --> 01:13:09.320]   We're going to have here "question" and "with a question".
[01:13:09.320 --> 01:13:12.360]   But before it, we're going to say, "answer the question with the help of the context."
[01:13:12.360 --> 01:13:14.160]   We're going to provide this to the language model.
[01:13:14.160 --> 01:13:18.080]   And we're going to say "context" and they're going to have the whole webpage.
[01:13:18.080 --> 01:13:20.800]   So suddenly now our question is going to be a lot bigger.
[01:13:20.800 --> 01:13:23.840]   Our prompt.
[01:13:23.840 --> 01:13:32.040]   So our prompt now contains the entire webpage, the whole Wikipedia page, followed by our
[01:13:32.040 --> 01:13:34.100]   question.
[01:13:34.100 --> 01:13:41.440]   And so now it says, "Jeremy Howd is an Australian data scientist, entrepreneur, and educator,
[01:13:41.440 --> 01:13:46.560]   known for his work in deep learning, co-founder of Fast.ai, teaches courses, develops software,
[01:13:46.560 --> 01:13:48.560]   conducts research, used to be..."
[01:13:48.560 --> 01:13:51.400]   Yeah, okay, it's perfect.
[01:13:51.400 --> 01:13:54.480]   So it's actually done a really good job.
[01:13:54.480 --> 01:14:01.560]   Like if somebody asked me to send them a, you know, hundred word bio, that would actually
[01:14:01.560 --> 01:14:04.080]   probably be better than I would have written myself.
[01:14:04.080 --> 01:14:10.180]   And you'll see even though I asked for 300 tokens, it actually got sent back the end
[01:14:10.180 --> 01:14:11.660]   of stream token.
[01:14:11.660 --> 01:14:17.160]   And so it knows to stop at this point.
[01:14:17.160 --> 01:14:22.980]   Well that's all very well, but how do we know to pass in the Jeremy Howard Wikipedia page?
[01:14:22.980 --> 01:14:29.660]   Well the way we know which Wikipedia page to pass in is that we can use another model
[01:14:29.660 --> 01:14:41.500]   to tell us which webpage or which document is the most useful for answering a question.
[01:14:41.500 --> 01:14:46.720]   And the way we do that is we can use something called sentence transformer, and we can use
[01:14:46.720 --> 01:14:54.220]   a special kind of model that's specifically designed to take a document and turn it into
[01:14:54.220 --> 01:15:01.580]   a bunch of activations where two documents that are similar will have similar activations.
[01:15:01.580 --> 01:15:03.780]   So let me just let me show you what I mean.
[01:15:03.780 --> 01:15:09.640]   What I'm going to do is I'm going to grab just the first paragraph of my Wikipedia page,
[01:15:09.640 --> 01:15:14.820]   and I'm going to grab the first paragraph of Tony Blair's Wikipedia page.
[01:15:14.820 --> 01:15:17.180]   Okay so we're pretty different people, right?
[01:15:17.180 --> 01:15:20.180]   This is just like a really simple small example.
[01:15:20.180 --> 01:15:25.860]   And I'm going to then call this model, I'm going to say encode, and I'm going to encode
[01:15:25.860 --> 01:15:33.620]   my Wikipedia first paragraph, Tony Blair's first paragraph, and the question which was
[01:15:33.620 --> 01:15:37.180]   who is Jeremy Howard.
[01:15:37.180 --> 01:15:47.500]   And it's going to pass back a 384 long vector of embeddings for the question, for me, and
[01:15:47.500 --> 01:15:49.300]   for Tony Blair.
[01:15:49.300 --> 01:15:56.260]   And what I can now do is I can calculate the similarity between the question and the Jeremy
[01:15:56.260 --> 01:15:58.660]   Howard Wikipedia page.
[01:15:58.660 --> 01:16:03.020]   And I can also do it for the question versus the Tony Blair Wikipedia page.
[01:16:03.020 --> 01:16:06.480]   And as you can see it's higher for me.
[01:16:06.480 --> 01:16:11.340]   And so that tells you that if you're trying to figure out what document to use to help
[01:16:11.340 --> 01:16:16.160]   you answer this question, better off using the Jeremy Howard Wikipedia page than the
[01:16:16.160 --> 01:16:20.540]   Tony Blair Wikipedia page.
[01:16:20.540 --> 01:16:26.940]   So if you had a few hundred documents you were thinking of using to give back to the
[01:16:26.940 --> 01:16:32.220]   model as context to help it answer a question, you could literally just pass them all through
[01:16:32.220 --> 01:16:37.980]   to encode, go through each one one at a time and see which is closest.
[01:16:37.980 --> 01:16:44.740]   When you've got thousands or millions of documents, you can use something called a vector database,
[01:16:44.740 --> 01:16:52.300]   where basically as a one-off thing you go through and you encode all of your documents.
[01:16:52.300 --> 01:16:57.820]   And so in fact there's lots of pre-built systems for this.
[01:16:57.820 --> 01:17:02.700]   Here's an example of one called H2O GPT.
[01:17:02.700 --> 01:17:12.780]   And this is just something that I've got running here on my computer.
[01:17:12.780 --> 01:17:15.420]   It's just an open source thing written in Python.
[01:17:15.420 --> 01:17:18.700]   It's sitting here running on port 7860.
[01:17:18.700 --> 01:17:22.660]   And so I've just gone to localhost 7860.
[01:17:22.660 --> 01:17:29.380]   And what I did was I just uploaded, I just clicked upload, and uploaded a bunch of papers.
[01:17:29.380 --> 01:17:31.820]   In fact I might be able to see it better.
[01:17:31.820 --> 01:17:33.660]   Yeah, here we go.
[01:17:33.660 --> 01:17:35.980]   A bunch of papers.
[01:17:35.980 --> 01:17:42.860]   And so, you know, we could look at, can we search?
[01:17:42.860 --> 01:17:43.860]   Yeah I can.
[01:17:43.860 --> 01:17:48.380]   So for example we can look at the ULM fit paper that Sir Bruder and I did.
[01:17:48.380 --> 01:17:55.180]   And you can see it's taken the PDF and turned it into, slightly crappily, a text format.
[01:17:55.180 --> 01:18:02.740]   And then it's created an embedding for each, you know, each section.
[01:18:02.740 --> 01:18:12.260]   So I could then ask it, you know, what is ULM fit?
[01:18:12.260 --> 01:18:15.340]   And I'll hit enter.
[01:18:15.340 --> 01:18:17.980]   And you can see here it's now actually saying "based on the information provided in the
[01:18:17.980 --> 01:18:18.980]   context".
[01:18:18.980 --> 01:18:21.020]   So it's showing us, it's been given some context.
[01:18:21.020 --> 01:18:22.160]   What context did it get?
[01:18:22.160 --> 01:18:25.260]   So here are the things that it found.
[01:18:25.260 --> 01:18:27.260]   Right?
[01:18:27.260 --> 01:18:31.660]   So it's being sent this context.
[01:18:31.660 --> 01:18:34.060]   This is kind of citations.
[01:18:34.060 --> 01:18:41.380]   "Goal of ULM fit proves the performance by leveraging the knowledge and adapting it to
[01:18:41.380 --> 01:18:45.740]   the specific task at hand".
[01:18:45.740 --> 01:18:50.940]   How, what techniques, be more specific.
[01:18:50.940 --> 01:18:55.180]   Does ULM fit?
[01:18:55.180 --> 01:18:56.180]   Let's see how it goes.
[01:18:56.180 --> 01:19:01.820]   Okay, there we go.
[01:19:01.820 --> 01:19:02.820]   So here's the three steps.
[01:19:02.820 --> 01:19:04.820]   Pre-train, fine-tune, fine-tune.
[01:19:04.820 --> 01:19:05.820]   Cool.
[01:19:05.820 --> 01:19:08.540]   So you can see it's not bad.
[01:19:08.540 --> 01:19:09.540]   Right?
[01:19:09.540 --> 01:19:10.540]   It's not amazing.
[01:19:10.540 --> 01:19:16.900]   Like, you know, the context in this particular case is pretty small.
[01:19:16.900 --> 01:19:22.580]   And it's, and in particular, if you think about how that embedding thing worked, you
[01:19:22.580 --> 01:19:25.420]   can't really use, like, the normal kind of follow-up.
[01:19:25.420 --> 01:19:31.780]   So for example, if I, so it says fine-tuning a classifier.
[01:19:31.780 --> 01:19:36.540]   So I could say, "What classifier is used?"
[01:19:36.540 --> 01:19:41.060]   Now the problem is that there's no context here being sent to the embedding model.
[01:19:41.060 --> 01:19:44.540]   So it's actually going to have no idea I'm talking about ULM fit.
[01:19:44.540 --> 01:19:47.020]   So generally speaking, it's going to do a terrible job.
[01:19:47.020 --> 01:19:50.700]   Yeah, see, it says used as a Roberta model, but it's not.
[01:19:50.700 --> 01:19:56.540]   But if I look at the sources, it's no longer actually referring to Howard and Ruta.
[01:19:56.540 --> 01:19:58.980]   So anyway, you can see the basic idea.
[01:19:58.980 --> 01:20:04.980]   This is called retrieval augmented generation, RAG.
[01:20:04.980 --> 01:20:12.100]   And it's a nifty approach, but you have to do it with some care.
[01:20:12.100 --> 01:20:18.140]   And so there are lots of these private GPT things out there.
[01:20:18.140 --> 01:20:28.940]   The H2O GPT web page does a fantastic job of listing lots of them and comparing.
[01:20:28.940 --> 01:20:38.940]   So as you can see, if you want to run a private GPT, there's no shortage of options.
[01:20:38.940 --> 01:20:42.020]   And you can have your retrieval augmented generation.
[01:20:42.020 --> 01:20:45.100]   I haven't tried, I've only tried this one, H2O GPT.
[01:20:45.100 --> 01:20:50.740]   I don't love it, it's all right.
[01:20:50.740 --> 01:20:56.100]   So finally, I want to talk about what's perhaps the most interesting option we have, which
[01:20:56.100 --> 01:20:58.360]   is to do our own fine tuning.
[01:20:58.360 --> 01:21:02.500]   And fine tuning is cool, because rather than just retrieving documents which might have
[01:21:02.500 --> 01:21:09.100]   useful context, we can actually change our model to behave based on the documents that
[01:21:09.100 --> 01:21:10.100]   we have available.
[01:21:10.100 --> 01:21:14.420]   I'm going to show you a really interesting example of fine tuning here.
[01:21:14.420 --> 01:21:22.140]   What we're going to do is we're going to fine tune using this no SQL data set.
[01:21:22.140 --> 01:21:32.900]   And it's got examples of like a schema for a table in a database, a question, and then
[01:21:32.900 --> 01:21:44.940]   the answer is the correct SQL to solve that question using that database schema.
[01:21:44.940 --> 01:21:51.540]   And so I'm hoping we could use this to create a, you know, a kind of, it could be a handy
[01:21:51.540 --> 01:21:59.380]   use, a handy tool for business users where they type some English question and SQL generated
[01:21:59.380 --> 01:22:00.780]   for them automatically.
[01:22:00.780 --> 01:22:05.820]   Don't know if it actually work in practice or not, but this is just a little fun idea
[01:22:05.820 --> 01:22:08.620]   I thought we'd try out.
[01:22:08.620 --> 01:22:14.580]   I know there's lots of startups and stuff out there trying to do this more seriously.
[01:22:14.580 --> 01:22:20.080]   But this is quite cool, because it actually got it working today in just a couple of hours.
[01:22:20.080 --> 01:22:27.880]   So what we do is we use the HuggingFace datasets library.
[01:22:27.880 --> 01:22:33.100]   And what that does, just like the HuggingFace hub has lots of models stored on it, HuggingFace
[01:22:33.100 --> 01:22:36.780]   datasets has lots of data sets stored on it.
[01:22:36.780 --> 01:22:41.700]   And so instead of using transformers, which is what we use to grab models, we use datasets.
[01:22:41.700 --> 01:22:47.300]   And we just pass in the name of the person and the name of their repo, and it grabs the
[01:22:47.300 --> 01:22:48.300]   data set.
[01:22:48.300 --> 01:22:50.460]   And so we can take a look at it.
[01:22:50.460 --> 01:22:55.460]   And it just has a training set with features.
[01:22:55.460 --> 01:23:02.740]   And so then I can have a look at the training set.
[01:23:02.740 --> 01:23:07.060]   So here's an example, which looks a bit like what we've just seen.
[01:23:07.060 --> 01:23:11.800]   So what we do now is we want to fine-tune a model.
[01:23:11.800 --> 01:23:16.220]   Now we can do that in a notebook from scratch.
[01:23:16.220 --> 01:23:18.620]   Takes I don't know, 100 or so lines of code.
[01:23:18.620 --> 01:23:19.740]   It's not too much.
[01:23:19.740 --> 01:23:24.620]   But given the time constraints here, and also like I thought, why not, why don't we just
[01:23:24.620 --> 01:23:26.820]   use something that's ready to go.
[01:23:26.820 --> 01:23:33.980]   So for example, there's something called Axolotl, which is quite nice, in my opinion.
[01:23:33.980 --> 01:23:34.980]   Here it is here.
[01:23:34.980 --> 01:23:38.500]   Another very nice open source piece of software.
[01:23:38.500 --> 01:23:41.740]   And again, you can just pip install it.
[01:23:41.740 --> 01:23:48.020]   And it's got things like GPTQ and 16-bit and so forth, ready to go.
[01:23:48.020 --> 01:23:56.340]   And so what I did was I, it basically has a whole bunch of examples of things that it
[01:23:56.340 --> 01:23:58.580]   already knows how to do.
[01:23:58.580 --> 01:23:59.740]   It's got Lama2 examples.
[01:23:59.740 --> 01:24:02.820]   So I copied the Lama2 example.
[01:24:02.820 --> 01:24:05.020]   And I created a SQL example.
[01:24:05.020 --> 01:24:09.860]   So I basically just told it, this is the path to the data set that I want.
[01:24:09.860 --> 01:24:13.140]   This is the type.
[01:24:13.140 --> 01:24:16.740]   And everything else pretty much I left the same.
[01:24:16.740 --> 01:24:22.900]   And then I just ran this command, which is from their readme, accelerate launch Axolotl,
[01:24:22.900 --> 01:24:24.380]   passed in my YAML.
[01:24:24.380 --> 01:24:28.780]   And that took about an hour on my GPU.
[01:24:28.780 --> 01:24:35.980]   And at the end of the hour, it had created a QLoraOut directory.
[01:24:35.980 --> 01:24:36.980]   Q stands for quantized.
[01:24:36.980 --> 01:24:39.580]   That's because I was creating a smaller quantized model.
[01:24:39.580 --> 01:24:41.780]   Lora, I'm not going to talk about today.
[01:24:41.780 --> 01:24:46.900]   But Lora is a very cool thing that basically, another thing that makes your models smaller
[01:24:46.900 --> 01:24:54.700]   and also handles, can use bigger models on smaller GPUs for training.
[01:24:54.700 --> 01:24:59.460]   So I trained it.
[01:24:59.460 --> 01:25:04.820]   And then I thought, okay, let's create our own one.
[01:25:04.820 --> 01:25:08.580]   So we're going to have this context.
[01:25:08.580 --> 01:25:19.060]   And this question, get the count of competition hosts by theme.
[01:25:19.060 --> 01:25:20.540]   And I'm not going to pass it an answer.
[01:25:20.540 --> 01:25:22.860]   So I'll just ignore that.
[01:25:22.860 --> 01:25:28.780]   So again, I found out what prompt they were using.
[01:25:28.780 --> 01:25:31.340]   And created a SQL prompt function.
[01:25:31.340 --> 01:25:32.940]   And so here's what I'm going to do.
[01:25:32.940 --> 01:25:37.300]   Use the following contextual information to answer the question.
[01:25:37.300 --> 01:25:39.700]   Prompt create table, so there's the context.
[01:25:39.700 --> 01:25:44.460]   Question list all competition hosts sorted in ascending order.
[01:25:44.460 --> 01:25:50.380]   And then I tokenized that.
[01:25:50.380 --> 01:25:53.420]   Called generate.
[01:25:53.420 --> 01:26:01.020]   And the answer was select count hosts comma theme from farm competition group by theme.
[01:26:01.020 --> 01:26:02.700]   That is correct.
[01:26:02.700 --> 01:26:05.860]   So I think that's pretty remarkable.
[01:26:05.860 --> 01:26:11.620]   We have just built, you know, so it took me like an hour to figure out how to do it.
[01:26:11.620 --> 01:26:15.300]   And then an hour to actually do the training.
[01:26:15.300 --> 01:26:24.260]   And at the end of that, we've actually got something which is converting pros into SQL
[01:26:24.260 --> 01:26:25.440]   based on a schema.
[01:26:25.440 --> 01:26:29.340]   So I think that's a really exciting idea.
[01:26:29.340 --> 01:26:36.440]   The only other thing I do want to briefly mention is doing stuff on Macs.
[01:26:36.440 --> 01:26:42.020]   If you've got a Mac, there's a couple of really good options.
[01:26:42.020 --> 01:26:46.300]   The options are MLC and llama.cpp, currently.
[01:26:46.300 --> 01:26:49.940]   MLC in particular, I think it's kind of underappreciated.
[01:26:49.940 --> 01:27:04.820]   It's a, you know, really nice project where you can run language models on literally iPhone,
[01:27:04.820 --> 01:27:08.740]   Android, web browsers, everything.
[01:27:08.740 --> 01:27:13.060]   It's really cool.
[01:27:13.060 --> 01:27:22.980]   And so I'm now actually on my Mac here, and I've got a tiny little Python program called
[01:27:22.980 --> 01:27:24.620]   chat.
[01:27:24.620 --> 01:27:36.100]   And it's going to import chat module, and it's going to import a discretized 7b.
[01:27:36.100 --> 01:27:40.340]   And that's going to ask the question, what is the meaning of life?
[01:27:40.340 --> 01:27:43.140]   So let's try it.
[01:27:43.140 --> 01:27:44.140]   Python chat.py.
[01:27:44.140 --> 01:27:48.380]   Again, I just installed this earlier today.
[01:27:48.380 --> 01:27:54.780]   I haven't done that much stuff on Macs before, but I was pretty impressed to see that it
[01:27:54.780 --> 01:27:59.100]   is doing a good job here.
[01:27:59.100 --> 01:28:04.180]   What is the meaning of life is complex and philosophical.
[01:28:04.180 --> 01:28:10.060]   Some people might find meaning in their relationships with others, their impact in the world, etc.
[01:28:10.060 --> 01:28:15.660]   Okay, and it's doing 9.6 tokens per second.
[01:28:15.660 --> 01:28:16.660]   So there you go.
[01:28:16.660 --> 01:28:20.020]   So there is running a model on a Mac.
[01:28:20.020 --> 01:28:24.900]   And then another option that you've probably heard about is llama.cpp.
[01:28:24.900 --> 01:28:32.980]   Llama.cpp runs on lots of different things as well, including Macs and also on CUDA.
[01:28:32.980 --> 01:28:36.660]   It uses a different format called gguf.
[01:28:36.660 --> 01:28:40.620]   And again, you can use it from Python, even though it's a CPP thing, it's got a Python
[01:28:40.620 --> 01:28:41.620]   wrapper.
[01:28:41.620 --> 01:28:47.580]   So you can just download, again, from Hugging Face, a gguf file.
[01:28:47.580 --> 01:28:52.940]   So you can just go through and there's lots of different ones.
[01:28:52.940 --> 01:28:54.540]   They're all documented as to what's what.
[01:28:54.540 --> 01:28:56.620]   You can pick how big a file you want.
[01:28:56.620 --> 01:28:58.340]   You can download it.
[01:28:58.340 --> 01:29:04.020]   And then you just say, okay, llama model_path=, pass in that gguf file.
[01:29:04.020 --> 01:29:08.140]   It spits out lots and lots and lots of gunk.
[01:29:08.140 --> 01:29:14.100]   And then you can say, okay, so if I called that LLM, you can then say LLM question, name
[01:29:14.100 --> 01:29:18.940]   the planets of the solar system, 32 tokens.
[01:29:18.940 --> 01:29:21.700]   And here we are.
[01:29:21.700 --> 01:29:23.620]   One Pluto, no longer considered a planet.
[01:29:23.620 --> 01:29:28.820]   Two Mercury, three Venus, four Earth, Mars, six, oh, and I've run out of tokens.
[01:29:28.820 --> 01:29:37.140]   So again, you know, it's just to show you here, there are all these different options.
[01:29:37.140 --> 01:29:43.840]   You know, I would say, you know, if you've got a NVIDIA graphics card and you're a reasonably
[01:29:43.840 --> 01:29:49.340]   capable Python programmer, you'd probably be wanting to use PyTorch and the Hugging
[01:29:49.340 --> 01:29:50.340]   Face ecosystem.
[01:29:50.340 --> 01:29:55.260]   But, you know, I think, you know, these things might change over time as well.
[01:29:55.260 --> 01:29:58.480]   And certainly a lot of stuff is coming into Llama pretty quickly now, and it's developing
[01:29:58.480 --> 01:29:59.920]   very fast.
[01:29:59.920 --> 01:30:07.460]   As you can see, there's a lot of stuff that you can do right now with language models.
[01:30:07.460 --> 01:30:12.380]   Particularly if you feel pretty comfortable as a Python programmer.
[01:30:12.380 --> 01:30:14.300]   I think it's a really exciting time to get involved.
[01:30:14.300 --> 01:30:17.740]   In some ways, it's a frustrating time to get involved.
[01:30:17.740 --> 01:30:23.580]   Because, you know, it's very early.
[01:30:23.580 --> 01:30:29.540]   And a lot of stuff has weird little edge cases, and it's tricky to install, and stuff like
[01:30:29.540 --> 01:30:32.300]   that.
[01:30:32.300 --> 01:30:34.780]   There's a lot of great Discord channels, however.
[01:30:34.780 --> 01:30:39.660]   Fast.ai have our own Discord channel, so feel free to just Google for Fast.ai Discord and
[01:30:39.660 --> 01:30:40.660]   drop in.
[01:30:40.660 --> 01:30:42.540]   We've got a channel called Generative.
[01:30:42.540 --> 01:30:47.540]   You feel free to ask any questions or tell us about what you're finding.
[01:30:47.540 --> 01:30:51.300]   Yeah, it's definitely something where you want to be getting help from other people
[01:30:51.300 --> 01:30:56.900]   on this journey, because it is very early days, and, you know, people are still figuring
[01:30:56.900 --> 01:30:58.640]   things out as we go.
[01:30:58.640 --> 01:31:03.740]   But I think it's an exciting time to be doing this stuff, and I'm really enjoying it.
[01:31:03.740 --> 01:31:09.140]   And I hope that this has given some of you a useful starting point on your own journey.
[01:31:09.140 --> 01:31:10.780]   So I hope you found this useful.
[01:31:10.780 --> 01:31:11.780]   Thanks for listening.
[01:31:11.780 --> 01:31:12.020]   Bye.

